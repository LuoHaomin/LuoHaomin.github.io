<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>luohaomin&#39;s blog – AI/智能计算软件优化/神经网络应用</title>
    <link>http://localhost:1313/tags/ai/%E6%99%BA%E8%83%BD%E8%AE%A1%E7%AE%97%E8%BD%AF%E4%BB%B6%E4%BC%98%E5%8C%96/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%BA%94%E7%94%A8/</link>
    <description>Recent content in AI/智能计算软件优化/神经网络应用 on luohaomin&#39;s blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    
	  <atom:link href="http://localhost:1313/tags/ai/%E6%99%BA%E8%83%BD%E8%AE%A1%E7%AE%97%E8%BD%AF%E4%BB%B6%E4%BC%98%E5%8C%96/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%BA%94%E7%94%A8/index.xml" rel="self" type="application/rss+xml" />
    
    
      
        
      
    
    
    <item>
      <title>神经网络应用</title>
      <link>http://localhost:1313/notes/learn/%E6%99%BA%E8%83%BD%E8%AE%A1%E7%AE%97%E8%BD%AF%E4%BB%B6%E4%BC%98%E5%8C%96/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%BA%94%E7%94%A8/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/notes/learn/%E6%99%BA%E8%83%BD%E8%AE%A1%E7%AE%97%E8%BD%AF%E4%BB%B6%E4%BC%98%E5%8C%96/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%BA%94%E7%94%A8/</guid>
      <description>
        
        
        &lt;h2&gt;1. 引言&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;1-引言&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#1-%e5%bc%95%e8%a8%80&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;h3&gt;1.1 深度学习基础与应用概览&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;11-深度学习基础与应用概览&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#11-%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e5%9f%ba%e7%a1%80%e4%b8%8e%e5%ba%94%e7%94%a8%e6%a6%82%e8%a7%88&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;本章的学习目标是将前一章中获得的&lt;strong&gt;神经网络基础知识&lt;/strong&gt;（如&lt;strong&gt;多层感知机&lt;/strong&gt;的正反向计算过程，以及基础&lt;strong&gt;优化方法&lt;/strong&gt;）应用到实际场景中。通过分析&lt;strong&gt;经典深度学习算法&lt;/strong&gt;，学习如何将基础神经网络应用于实际场景，并逐步优化以实现&lt;strong&gt;工业级应用&lt;/strong&gt;，最终目标是使机器能更好地理解和服务人类。&lt;/p&gt;
&lt;h3&gt;1.2 输入类型与任务匹配&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;12-输入类型与任务匹配&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#12-%e8%be%93%e5%85%a5%e7%b1%bb%e5%9e%8b%e4%b8%8e%e4%bb%bb%e5%8a%a1%e5%8c%b9%e9%85%8d&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;人类获取的信息可以分为两大主要类型，深度学习模型根据这些类型及其对应的任务，采用不同的神经网络结构：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;图像信息&lt;/strong&gt;：
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;任务&lt;/strong&gt;：理解图像内容（例如，识别图像中的物体、场景等）。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;方法&lt;/strong&gt;：主要通过&lt;strong&gt;卷积神经网络 (CNN)&lt;/strong&gt; 来处理这类任务。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;序列信息&lt;/strong&gt;：
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;任务&lt;/strong&gt;：理解语音、文字或视频内容（这些数据具有时间或逻辑上的序列依赖性）。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;方法&lt;/strong&gt;：主要通过&lt;strong&gt;循环神经网络 (RNN)&lt;/strong&gt; 来处理这类任务。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;1.3 归纳偏置 (Inductive Bias)&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;13-归纳偏置-inductive-bias&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#13-%e5%bd%92%e7%ba%b3%e5%81%8f%e7%bd%ae-inductive-bias&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;h4&gt;1.3.1 定义与哲学渊源&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;131-定义与哲学渊源&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#131-%e5%ae%9a%e4%b9%89%e4%b8%8e%e5%93%b2%e5%ad%a6%e6%b8%8a%e6%ba%90&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;定义&lt;/strong&gt;：&lt;strong&gt;归纳偏置 (Inductive Bias)&lt;/strong&gt; 是指&lt;strong&gt;模型结构本身所蕴含的限制、假设和偏向&lt;/strong&gt;。它决定了模型在面对未知数据时，倾向于做出哪种类型的预测，或者说，它对学习任务设定了一种“先验”的约束。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;哲学渊源&lt;/strong&gt;：
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;洛克 (John Locke)&lt;/strong&gt; 的《人类理解论》中的&lt;strong&gt;经验主义&lt;/strong&gt;：洛克认为人的心灵初始状态像一块“白板”，精神内容完全来源于经验。这可以类比于一个模型，如果没有归纳偏置，它将完全依赖从数据中学习到的模式。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;康德 (Immanuel Kant)&lt;/strong&gt; 的《纯粹理性批判》中的&lt;strong&gt;综合先验判断&lt;/strong&gt;：康德认为存在不来源于感官经验、而是来源于心智的先天结构的知识（如时空、逻辑、数学等）。他提出，我们通过结合&lt;strong&gt;先天知识&lt;/strong&gt;和&lt;strong&gt;经验&lt;/strong&gt;来认识世界。这可以类比于归纳偏置，即模型内置的结构（先天知识）使其能够更好地处理某些类型的数据（经验），从而更有效地学习。康德认为，正是因为我们先天具备了认识三维时空、逻辑和数学的大脑结构，我们才能更好地认识世界。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;1.3.2 神经网络模型与归纳偏置&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;132-神经网络模型与归纳偏置&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#132-%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c%e6%a8%a1%e5%9e%8b%e4%b8%8e%e5%bd%92%e7%ba%b3%e5%81%8f%e7%bd%ae&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;正如康德所言，大脑的先天结构决定了其认识世界的方式，不同的神经网络模型结构也具有&lt;strong&gt;不同的归纳偏置&lt;/strong&gt;。这意味着：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;每种模型结构都对输入数据和学习任务做出了特定的&lt;strong&gt;假设&lt;/strong&gt;。&lt;/li&gt;
&lt;li&gt;这些假设使得模型在处理&lt;strong&gt;特定类型的数据&lt;/strong&gt;或解决&lt;strong&gt;特定类型的问题&lt;/strong&gt;时效率更高、性能更好。&lt;/li&gt;
&lt;li&gt;例如，&lt;strong&gt;卷积神经网络 (CNN)&lt;/strong&gt; 具有&lt;strong&gt;局部连接&lt;/strong&gt;和&lt;strong&gt;权重共享&lt;/strong&gt;的归纳偏置，这使其天然适合处理具有空间局部性和平移不变性的图像数据。而&lt;strong&gt;循环神经网络 (RNN)&lt;/strong&gt; 的&lt;strong&gt;循环结构&lt;/strong&gt;使其适合处理具有时间依赖性的序列数据。因此，&lt;strong&gt;选择合适的神经网络模型结构&lt;/strong&gt;对于解决特定任务至关重要，因为它能够利用与任务匹配的归纳偏置来更有效地学习。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;2. 适合图像处理的卷积神经网络&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;2-适合图像处理的卷积神经网络&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#2-%e9%80%82%e5%90%88%e5%9b%be%e5%83%8f%e5%a4%84%e7%90%86%e7%9a%84%e5%8d%b7%e7%a7%af%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;h3&gt;2.1 卷积神经网络概述&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;21-卷积神经网络概述&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#21-%e5%8d%b7%e7%a7%af%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c%e6%a6%82%e8%bf%b0&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;h4&gt;2.1.1 发展历史&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;211-发展历史&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#211-%e5%8f%91%e5%b1%95%e5%8e%86%e5%8f%b2&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;对卷积神经网络 (CNN) 的研究可以追溯到&lt;strong&gt;日本学者福岛邦彦&lt;/strong&gt;在 &lt;strong&gt;1979 和 1980 年&lt;/strong&gt;提出的 &lt;strong&gt;Neocognition 模型&lt;/strong&gt;。这个模型参照了生物的视觉皮层 (visual cortex) 设计，是现代CNN的早期雏形。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;AlexNet&lt;/strong&gt; 在 &lt;strong&gt;ILSVRC2012&lt;/strong&gt; (ImageNet Large Scale Visual Recognition Challenge 2012) 图像分类竞赛中取得&lt;strong&gt;巨大成功&lt;/strong&gt;。它显著提高了图像分类的准确率，并标志着卷积神经网络在图像分类领域获得快速发展，成为主流方法。&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;2.1.2 全连接网络的挑战&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;212-全连接网络的挑战&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#212-%e5%85%a8%e8%bf%9e%e6%8e%a5%e7%bd%91%e7%bb%9c%e7%9a%84%e6%8c%91%e6%88%98&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;问题背景：&lt;/strong&gt; 对于计算机视觉任务，例如识别图像中的物体，传统的多层感知机 (MLP) 或全连接网络 (Fully Connected Network, FCN) 会面临巨大的参数量挑战。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;示例分析 (原始资料第7页和第8页图示的文字描述):&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;小图像示例 (32x32x3):&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;输入图像大小：32x32 像素，通道数 RGB 为 3。&lt;/li&gt;
&lt;li&gt;输入数据量：$32 \times 32 \times 3 = 3072$ 个特征。&lt;/li&gt;
&lt;li&gt;如果第一层隐层神经元有 100 个，那么连接输入层和第一层隐层的权重数量将是 $3072 \times 100 = 307200$。这已经是相当大的参数量。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;大图像示例 (1024x1024x3):&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;实际场景中，图像尺寸往往更大，例如 $1024 \times 1024$ 像素，通道数 3。&lt;/li&gt;
&lt;li&gt;如果第一层隐层神经元有 1000 个 (这还是一个相对较小的数字)，那么仅仅是第一层的权重数量将达到 $1024 \times 1024 \times 3 \times 1000 \approx 3 \times 10^9$ (即 30 亿) 的量级。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;挑战总结：&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;参数量过大：&lt;/strong&gt; 如此庞大的权重数量会导致模型训练困难，计算资源消耗巨大。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;过拟合 (Overfitting)：&lt;/strong&gt; 过多的参数使得模型容易过度学习训练数据中的噪声和特定模式，导致在新数据上泛化能力差。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;解决方案：&lt;/strong&gt; 卷积神经网络通过其特有的 &lt;strong&gt;局部连接&lt;/strong&gt; 和 &lt;strong&gt;权重共享&lt;/strong&gt; 机制，可以&lt;strong&gt;有效减少权重数量&lt;/strong&gt;，从而解决全连接网络在处理高维图像数据时面临的挑战，并为构建更深的网络结构提供了可能。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;2.2 卷积层 (Convolutional Layer)&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;22-卷积层-convolutional-layer&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#22-%e5%8d%b7%e7%a7%af%e5%b1%82-convolutional-layer&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;h4&gt;2.2.1 核心特征&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;221-核心特征&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#221-%e6%a0%b8%e5%bf%83%e7%89%b9%e5%be%81&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;卷积层是CNN的核心组成部分，其引入的两个重要特征极大地优化了图像处理：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;局部连接 (Local Connectivity)：&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;传统全连接：&lt;/strong&gt; 每个神经元都与前一层的所有神经元连接。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;卷积层：&lt;/strong&gt; 卷积层中的每个神经元（即输出特征图上的一个点）只与其输入特征图（或原始图像）的&lt;strong&gt;局部区域&lt;/strong&gt;相连接。这个局部区域的大小由&lt;strong&gt;滤波器 (filter/kernel)&lt;/strong&gt; 的尺寸决定。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;优势：&lt;/strong&gt; 这种设计模仿了生物视觉系统对局部特征的感知，例如边缘、纹理等，并且大大减少了连接数量。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;权重共享 (Weight Sharing)：&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;传统全连接：&lt;/strong&gt; 不同的连接使用不同的权重。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;卷积层：&lt;/strong&gt; 一个滤波器（即一组权重）在输入特征图上&lt;strong&gt;滑动&lt;/strong&gt;，对所有局部区域执行相同的卷积操作。这意味着&lt;strong&gt;同一组权重被输入特征图的所有局部区域共享&lt;/strong&gt;。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;优势：&lt;/strong&gt; 这种共享机制使得模型能够检测到图像中&lt;strong&gt;任何位置&lt;/strong&gt;的相同特征（例如，无论一只猫的眼睛出现在图像的左上角还是右下角，都可以被同一个滤波器检测到）。同时，它&lt;strong&gt;进一步减少了模型的总参数量&lt;/strong&gt;，有效避免过拟合，并使得构建更深层次的网络成为可能。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;2.2.2 卷积运算&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;222-卷积运算&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#222-%e5%8d%b7%e7%a7%af%e8%bf%90%e7%ae%97&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;本质：&lt;/strong&gt; 卷积运算在神经网络中实际是计算输入矩阵与滤波器矩阵之间的&lt;strong&gt;内积 (相关系数)&lt;/strong&gt;，用于提取特征。&lt;/li&gt;
&lt;/ul&gt;
&lt;h5&gt;2.2.2.1 数学定义&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;2221-数学定义&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#2221-%e6%95%b0%e5%ad%a6%e5%ae%9a%e4%b9%89&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h5&gt;&lt;ul&gt;
&lt;li&gt;在数学上，一维离散卷积的定义为：
$$y[n] = \sum_{i=-\infty}^{\infty} x[i]h[n-i] = x[n] * h[n]$$
其中，$x[n]$ 是输入信号，$h[n]$ 是卷积核 (滤波器)，$y[n]$ 是输出信号。&lt;/li&gt;
&lt;li&gt;在卷积神经网络中，这个概念被推广到二维图像，通常是输入数据与一个小的矩阵 (滤波器) 进行元素乘积求和的操作，然后滑动到下一个区域。&lt;/li&gt;
&lt;/ul&gt;
&lt;h5&gt;2.2.2.2 多输入单输出&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;2222-多输入单输出&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#2222-%e5%a4%9a%e8%be%93%e5%85%a5%e5%8d%95%e8%be%93%e5%87%ba&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h5&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;过程描述 (原始资料第13页图示的文字描述):&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;假设输入是一个 $6 \times 6 \times 3$ 的特征图 (例如，RGB 图像，6x6 像素，3 个颜色通道)。&lt;/li&gt;
&lt;li&gt;使用一个 $3 \times 3 \times 3$ 的滤波器 (这里，滤波器的深度必须与输入特征图的通道数相匹配)。&lt;/li&gt;
&lt;li&gt;这个滤波器在输入特征图的每个通道上分别进行卷积操作，然后将三个通道的卷积结果&lt;strong&gt;相加&lt;/strong&gt;，得到一个单一的输出特征图（例如 $4 \times 4$）。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;关键点：&lt;/strong&gt; 尽管输入有多个通道，但一个滤波器最终只生成一个输出通道。这意味着，如果我们需要多个输出特征图 (检测多种特征)，就需要使用多个不同的滤波器。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;【若需查看原始图片详情，请参考原文中的“多输入特征图单输出特征图卷积运算示意图”】&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h5&gt;2.2.2.3 边缘检测示例&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;2223-边缘检测示例&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#2223-%e8%be%b9%e7%bc%98%e6%a3%80%e6%b5%8b%e7%a4%ba%e4%be%8b&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h5&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;核心思想：&lt;/strong&gt; 不同的滤波器 (卷积核) 可以用来检测图像中的不同特征，例如垂直边缘、水平边缘、对角线边缘等。这是因为滤波器中的权重可以被训练成特定的模式，当与图像中的对应模式进行内积运算时，会产生较高的激活值。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;示例一：垂直边缘检测 (原始资料第16页图示的文字描述):&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;滤波器示例：&lt;/strong&gt;
&lt;div class=&#34;hextra-code-block hx:relative hx:mt-6 hx:first:mt-0 hx:group/code&#34;&gt;

&lt;div&gt;&lt;pre&gt;&lt;code&gt;1  0  -1
1  0  -1
1  0  -1&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;hextra-code-copy-btn-container hx:opacity-0 hx:transition hx:group-hover/code:opacity-100 hx:flex hx:gap-1 hx:absolute hx:m-[11px] hx:right-0 hx:top-0&#34;&gt;
  &lt;button
    class=&#34;hextra-code-copy-btn hx:group/copybtn hx:cursor-pointer hx:transition-all hx:active:opacity-50 hx:bg-primary-700/5 hx:border hx:border-black/5 hx:text-gray-600 hx:hover:text-gray-900 hx:rounded-md hx:p-1.5 hx:dark:bg-primary-300/10 hx:dark:border-white/10 hx:dark:text-gray-400 hx:dark:hover:text-gray-50&#34;
    title=&#34;Copy code&#34;
  &gt;
    &lt;div class=&#34;hextra-copy-icon hx:group-[.copied]/copybtn:hidden hx:pointer-events-none hx:h-4 hx:w-4&#34;&gt;&lt;/div&gt;
&lt;div class=&#34;hextra-success-icon hx:hidden hx:group-[.copied]/copybtn:block hx:pointer-events-none hx:h-4 hx:w-4&#34;&gt;&lt;/div&gt;
  &lt;/button&gt;
&lt;/div&gt;
&lt;/div&gt;
这个滤波器左右两侧是正负对称的，中间为零。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;输入图像示例：&lt;/strong&gt; 图像左侧是高亮度区域 (例如 10)，右侧是低亮度区域 (例如 0)，中间是过渡区域。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;卷积结果：&lt;/strong&gt; 当这个滤波器滑动到垂直边缘 (亮度从高到低变化) 时，其输出值会显著增大，从而“检测”到垂直边缘。例如，当滤波器跨越一个亮到暗的垂直边界时，它会将左侧亮区乘以正值，右侧暗区乘以负值，求和后产生较大的绝对值。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;示例二：对角线边缘检测 (原始资料第16页图示的文字描述):&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;滤波器示例：&lt;/strong&gt;
&lt;div class=&#34;hextra-code-block hx:relative hx:mt-6 hx:first:mt-0 hx:group/code&#34;&gt;

&lt;div&gt;&lt;pre&gt;&lt;code&gt;1  1  0
1  0  -1
0  -1 -1&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;hextra-code-copy-btn-container hx:opacity-0 hx:transition hx:group-hover/code:opacity-100 hx:flex hx:gap-1 hx:absolute hx:m-[11px] hx:right-0 hx:top-0&#34;&gt;
  &lt;button
    class=&#34;hextra-code-copy-btn hx:group/copybtn hx:cursor-pointer hx:transition-all hx:active:opacity-50 hx:bg-primary-700/5 hx:border hx:border-black/5 hx:text-gray-600 hx:hover:text-gray-900 hx:rounded-md hx:p-1.5 hx:dark:bg-primary-300/10 hx:dark:border-white/10 hx:dark:text-gray-400 hx:dark:hover:text-gray-50&#34;
    title=&#34;Copy code&#34;
  &gt;
    &lt;div class=&#34;hextra-copy-icon hx:group-[.copied]/copybtn:hidden hx:pointer-events-none hx:h-4 hx:w-4&#34;&gt;&lt;/div&gt;
&lt;div class=&#34;hextra-success-icon hx:hidden hx:group-[.copied]/copybtn:block hx:pointer-events-none hx:h-4 hx:w-4&#34;&gt;&lt;/div&gt;
  &lt;/button&gt;
&lt;/div&gt;
&lt;/div&gt;
这个滤波器呈现对角线的正负值分布。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;卷积结果：&lt;/strong&gt; 类似地，当这个滤波器滑动到图像中的对角线边缘时，会产生较高的响应，实现对角线边缘的检测。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;code&gt;【若需查看原始图片详情，请参考原文中的“垂直边缘检测与对角线边缘检测示例图”】&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;2.2.3 边界扩充 (Padding) 与卷积步长 (Stride)&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;223-边界扩充-padding-与卷积步长-stride&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#223-%e8%be%b9%e7%95%8c%e6%89%a9%e5%85%85-padding-%e4%b8%8e%e5%8d%b7%e7%a7%af%e6%ad%a5%e9%95%bf-stride&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;这两个参数是卷积层中控制输出特征图尺寸和信息保留的关键。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;边界扩充 (Padding)：&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;定义：&lt;/strong&gt; 在进行卷积操作之前，在输入图像或特征图的&lt;strong&gt;边界周围添加额外的像素&lt;/strong&gt;。这些像素通常填充为 0 或边缘像素的重复值。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;目的：&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;防止特征图持续减小：&lt;/strong&gt; 随着多层卷积，特征图的尺寸会逐渐缩小。Padding 可以保持特征图的尺寸不变或减缓其缩小速度，从而允许构建更深的网络。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;强化边缘信息：&lt;/strong&gt; 图像边缘的像素点在没有Padding的情况下，只参与少数几次卷积计算。通过Padding，边缘像素也能被充分利用，避免边缘信息丢失，因为边缘信息往往对图像识别很重要。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;卷积步长 (Stride)：&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;定义：&lt;/strong&gt; 滤波器在输入特征图上滑动时，每次移动的像素点个数。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;影响：&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;控制输出尺寸：&lt;/strong&gt; 步长越大，滤波器跳过的区域越多，输出特征图的尺寸就越小。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;下采样：&lt;/strong&gt; 当步长大于 1 时，实际上执行了一种形式的下采样 (down-sampling)，减少了计算量，但可能损失一些细节。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;输出特征图尺寸计算公式：&lt;/strong&gt;
给定输入尺寸 $W_i \times H_i$，滤波器尺寸 $K \times K$，Padding $p$，步长 $s$。
输出特征图的尺寸 $W_o \times H_o$ 为：
$$W_o = \frac{W_i + 2p - K}{s} + 1$$
$$H_o = \frac{H_i + 2p - K}{s} + 1$$
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;示例 (原始资料第17页图示的文字描述):&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;输入 $5 \times 5$，滤波器 $3 \times 3$，Padding $p=1$，步长 $s=2$。&lt;/li&gt;
&lt;li&gt;$W_o = (5 + 2 \times 1 - 3)/2 + 1 = (5+2-3)/2 + 1 = 4/2 + 1 = 2+1=3$&lt;/li&gt;
&lt;li&gt;$H_o = (5 + 2 \times 1 - 3)/2 + 1 = 3$&lt;/li&gt;
&lt;li&gt;因此，输出特征图尺寸为 $3 \times 3$。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;code&gt;【若需查看原始图片详情，请参考原文中的“卷积步长与Padding示例图”】&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;2.2.4 多输入多输出&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;224-多输入多输出&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#224-%e5%a4%9a%e8%be%93%e5%85%a5%e5%a4%9a%e8%be%93%e5%87%ba&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;过程描述 (原始资料第18页图示的文字描述):&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;如果输入是 $W_i \times H_i \times C_i$ (高、宽、输入通道数)。&lt;/li&gt;
&lt;li&gt;我们使用多个滤波器。每个滤波器 (例如 $K \times K \times C_i$) 作用于输入特征图，生成一个输出通道 (如前所述)。&lt;/li&gt;
&lt;li&gt;如果我们有 $C_o$ 个不同的滤波器，那么它们会分别生成 $C_o$ 个输出通道。&lt;/li&gt;
&lt;li&gt;最终的输出特征图尺寸将是 $W_o \times H_o \times C_o$ (高、宽、输出通道数)。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;关键点：&lt;/strong&gt; &lt;strong&gt;不同的滤波器可以检测不同的特征&lt;/strong&gt;。例如，一个滤波器可能检测垂直边缘，另一个检测水平边缘，第三个检测特定颜色模式。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;【若需查看原始图片详情，请参考原文中的“多输入特征图多输出特征图卷积运算示意图”】&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;2.2.5 参数总结&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;225-参数总结&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#225-%e5%8f%82%e6%95%b0%e6%80%bb%e7%bb%93&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;输入：&lt;/strong&gt; $W_i \times H_i \times C_i$ (宽度、高度、输入通道数)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;输出：&lt;/strong&gt; $W_o \times H_o \times C_o$ (宽度、高度、输出通道数)
&lt;ul&gt;
&lt;li&gt;$W_o = \frac{W_i + 2p - K}{s} + 1$&lt;/li&gt;
&lt;li&gt;$H_o = \frac{H_i + 2p - K}{s} + 1$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;滤波器 (Filter/Kernel)：&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;可训练参数&lt;/strong&gt;。每个滤波器的大小通常为 $K \times K \times C_i$。&lt;/li&gt;
&lt;li&gt;卷积层中共有 $C_o$ 个这样的滤波器，它们共同生成输出的 $C_o$ 个通道。&lt;/li&gt;
&lt;li&gt;因此，一个卷积层中滤波器的总参数量为 $C_o \times K \times K \times C_i$。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;偏置 (Bias)：&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;可训练参数&lt;/strong&gt;。每个输出通道通常对应一个偏置项。&lt;/li&gt;
&lt;li&gt;总偏置参数量为 $1 \times 1 \times C_o$ (每个输出通道一个偏置)。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;作用：&lt;/strong&gt; 偏置项允许分类器偏离激活函数原点，增加模型的灵活性，使其能够更好地拟合数据。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;激活函数 (Activation Function)：&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;卷积操作后通常会接着一个非线性激活函数 (如 ReLU)，为网络引入非线性能力，使其能够学习更复杂的模式。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;超参数：&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;步长 (stride)：&lt;/strong&gt; 滤波器滑动的步长。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;填充 (pad)：&lt;/strong&gt; 边界填充的大小。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;滤波器尺寸 (K)：&lt;/strong&gt; 滤波器的高度和宽度。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;输出通道数 (Co)：&lt;/strong&gt; 卷积层生成的特征图通道数，也代表了滤波器/特征的数量。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;2.3 池化层 (Pooling Layer)&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;23-池化层-pooling-layer&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#23-%e6%b1%a0%e5%8c%96%e5%b1%82-pooling-layer&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;h4&gt;2.3.1 作用与类型&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;231-作用与类型&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#231-%e4%bd%9c%e7%94%a8%e4%b8%8e%e7%b1%bb%e5%9e%8b&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;作用：&lt;/strong&gt; 池化层的主要功能是&lt;strong&gt;主动减小特征图的尺寸&lt;/strong&gt;，从而：
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;减少参数的数量和计算量：&lt;/strong&gt; 降低网络的复杂度。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;控制过拟合 (Overfitting)：&lt;/strong&gt; 通过减少特征维度，使模型对输入的变化更具鲁棒性。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;提取主要特征：&lt;/strong&gt; 保留最重要的信息。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;特点：&lt;/strong&gt; 池化层&lt;strong&gt;不引入额外的参数&lt;/strong&gt;，因为它执行的是固定的统计操作 (如取最大值、平均值)。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;主要类型：&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;最大池化 (Max Pooling)：&lt;/strong&gt; 从池化区域中选择最大的元素作为输出。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;平均池化 (Average Pooling)：&lt;/strong&gt; 计算池化区域中所有元素的平均值作为输出。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;L2 池化 (L2 Pooling)：&lt;/strong&gt; 计算池化区域中元素平方和的平方根作为输出。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;2.3.2 Max Pooling 特点&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;232-max-pooling-特点&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#232-max-pooling-%e7%89%b9%e7%82%b9&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;过程描述 (原始资料第20页图示的文字描述):&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;假设有一个 $6 \times 6$ 的输入特征图。&lt;/li&gt;
&lt;li&gt;使用一个 $2 \times 2$ 的滤波器 (这里称为池化窗口)。&lt;/li&gt;
&lt;li&gt;设置 Padding $p=0$，步长 $s=2$。&lt;/li&gt;
&lt;li&gt;Max Pooling 在每个 $2 \times 2$ 的窗口内，选择最大的数值作为输出。&lt;/li&gt;
&lt;li&gt;例如，在输入特征图的左上角 $2 \times 2$ 区域 &lt;code&gt;[2, 3; 7, 4]&lt;/code&gt; 中，最大值是 7，所以输出为 7。&lt;/li&gt;
&lt;li&gt;输出特征图尺寸将变为 $3 \times 3$。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;优势：&lt;/strong&gt; Max Pooling 能够&lt;strong&gt;保留特征的最大值&lt;/strong&gt;，这意味着它主要关注每个区域中响应最强的特征。这有助于&lt;strong&gt;提高提取特征的鲁棒性&lt;/strong&gt;，使其对特征在输入中的微小位置变化不那么敏感 (即平移不变性)。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;【若需查看原始图片详情，请参考原文中的“Max Pooling 示例图”】&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;2.4 全连接层 (Fully Connected Layer)&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;24-全连接层-fully-connected-layer&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#24-%e5%85%a8%e8%bf%9e%e6%8e%a5%e5%b1%82-fully-connected-layer&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;h4&gt;2.4.1 作用&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;241-作用&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#241-%e4%bd%9c%e7%94%a8&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;在卷积神经网络中，卷积层和池化层主要充当&lt;strong&gt;特征提取器&lt;/strong&gt;，它们从原始输入中学习并提取出抽象的、高层次的特征表示。&lt;/li&gt;
&lt;li&gt;全连接层 (FC) 则作为网络的&lt;strong&gt;分类器&lt;/strong&gt;。它将前一层 (通常是最后一个池化层或卷积层) 提取到的高维特征图展平 (flatten) 成&lt;strong&gt;一维特征向量&lt;/strong&gt;。&lt;/li&gt;
&lt;li&gt;这个一维特征向量包含了图像中所有重要的特征信息，然后被全连接层进一步处理，最终&lt;strong&gt;映射到各个类别的概率&lt;/strong&gt;。&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;2.4.2 Softmax 激活函数&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;242-softmax-激活函数&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#242-softmax-%e6%bf%80%e6%b4%bb%e5%87%bd%e6%95%b0&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;位置：&lt;/strong&gt; Softmax 函数通常作为&lt;strong&gt;网络的最后一层&lt;/strong&gt;，尤其是在执行&lt;strong&gt;多类别分类&lt;/strong&gt;任务时。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;作用：&lt;/strong&gt; 它将全连接层输出的原始分数 (logits) 转换成一个&lt;strong&gt;概率分布&lt;/strong&gt;。这个概率分布的所有元素都在 0 到 1 之间，并且它们的和为 1。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;特点：&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;归一化：&lt;/strong&gt; 将任意实数值归一化为概率值。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;凸显最大值：&lt;/strong&gt; Softmax 函数的指数特性会&lt;strong&gt;凸显其中最大的值&lt;/strong&gt;，同时&lt;strong&gt;抑制远低于最大值的其他分量&lt;/strong&gt;。这意味着，模型对某个类别的置信度越高，该类别的输出概率就越接近 1，而其他类别的概率则趋近于 0。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;公式：&lt;/strong&gt; 对于输入向量 $z = [z_1, z_2, \dots, z_n]$，Softmax 函数的输出 $f(z)_j$ (即第 $j$ 个类别的概率) 为：
$$f(z)&lt;em&gt;j = \frac{e^{z_j}}{\sum&lt;/em&gt;{i=0}^{n} e^{z_i}}$$
其中 $n$ 是类别的总数。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;2.5 卷积神经网络总体结构&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;25-卷积神经网络总体结构&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#25-%e5%8d%b7%e7%a7%af%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c%e6%80%bb%e4%bd%93%e7%bb%93%e6%9e%84&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;h4&gt;2.5.1 层排列规律&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;251-层排列规律&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#251-%e5%b1%82%e6%8e%92%e5%88%97%e8%a7%84%e5%be%8b&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;典型的卷积神经网络由以下几种层构成：
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;卷积层 (Conv Layer)：&lt;/strong&gt; 负责特征提取。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;激活函数 (ReLU)：&lt;/strong&gt; 通常紧跟在卷积层之后，引入非线性。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;池化层 (Pool Layer)：&lt;/strong&gt; 负责下采样和特征聚合。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;全连接层 (FC Layer)：&lt;/strong&gt; 负责分类。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Softmax 层：&lt;/strong&gt; 通常是网络的最后一层，输出分类概率。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;常见排列方式 (原始资料第22页图示的文字描述):&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;一个常见的模式是重复组合 &lt;strong&gt;Conv + ReLU&lt;/strong&gt;，然后接一个 &lt;strong&gt;Pool&lt;/strong&gt; 层。&lt;/li&gt;
&lt;li&gt;在特征提取部分完成后，通常会接一个或多个 &lt;strong&gt;FC + ReLU&lt;/strong&gt; 层。&lt;/li&gt;
&lt;li&gt;最后是一个 &lt;strong&gt;FC&lt;/strong&gt; 层，然后是 &lt;strong&gt;Softmax&lt;/strong&gt; 层，输出最终的分类结果。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;示例：&lt;/strong&gt; 如果设置重复次数 N=3, M=1, P=2 (N次 Conv+ReLU，M次Pool，P次FC+ReLU)，那么网络结构可能为：
&lt;code&gt;Input → (Conv + ReLU) x3 → Pool → (FC + ReLU) x2 → FC → Softmax (Output)&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;code&gt;【若需查看原始图片详情，请参考原文中的“卷积神经网络层排列规律示意图”】&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;2.5.2 经典网络示例 (VGG16)&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;252-经典网络示例-vgg16&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#252-%e7%bb%8f%e5%85%b8%e7%bd%91%e7%bb%9c%e7%a4%ba%e4%be%8b-vgg16&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;VGG16 结构概览 (原始资料第23页图示的文字描述):&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;VGG16 是一个经典的深度卷积神经网络，以其简洁和深度而闻名。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;输入：&lt;/strong&gt; 224x224 像素的 RGB 图像。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;特征提取部分 (由多个卷积层和池化层组成)：&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Conv1 (2层):&lt;/strong&gt; 两个 $3 \times 3$ 卷积层，输出 64 个通道。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Maxpool1:&lt;/strong&gt; 最大池化层，尺寸减半 (112x112)。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Conv2 (2层):&lt;/strong&gt; 两个 $3 \times 3$ 卷积层，输出 128 个通道。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Maxpool2:&lt;/strong&gt; 最大池化层，尺寸减半 (56x56)。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Conv3 (3层):&lt;/strong&gt; 三个 $3 \times 3$ 卷积层，输出 256 个通道。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Maxpool3:&lt;/strong&gt; 最大池化层，尺寸减半 (28x28)。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Conv4 (3层):&lt;/strong&gt; 三个 $3 \times 3$ 卷积层，输出 512 个通道。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Maxpool4:&lt;/strong&gt; 最大池化层，尺寸减半 (14x14)。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Conv5 (3层):&lt;/strong&gt; 三个 $3 \times 3$ 卷积层，输出 512 个通道。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Maxpool5:&lt;/strong&gt; 最大池化层，尺寸减半 (7x7)。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;分类部分 (由全连接层组成)：&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;FC1:&lt;/strong&gt; 4096 个神经元的全连接层。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;FC2:&lt;/strong&gt; 4096 个神经元的全连接层。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;FC3:&lt;/strong&gt; 1000 个神经元的全连接层 (对应 ImageNet 的 1000 个类别)。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Softmax:&lt;/strong&gt; 最后一层，输出 1000 个类别的概率。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;VGG 的设计哲学：&lt;/strong&gt; VGG 网络大量使用了小尺寸 ($3 \times 3$) 的卷积核，通过堆叠多个这样的卷积层来增加网络的深度，从而学习到更复杂的特征，同时保持较小的感受野。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;【若需查看原始图片详情，请参考原文中的“VGG16网络结构图”】&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;2.5.3 神经网络可视化 (浅层与深层特征)&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;253-神经网络可视化-浅层与深层特征&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#253-%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c%e5%8f%af%e8%a7%86%e5%8c%96-%e6%b5%85%e5%b1%82%e4%b8%8e%e6%b7%b1%e5%b1%82%e7%89%b9%e5%be%81&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;概念：&lt;/strong&gt; 神经网络可视化技术可以帮助我们理解网络内部各层学习到的特征。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;观察结果：&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;浅层 (例如 VGG16 的 Conv1)：&lt;/strong&gt; 倾向于学习到&lt;strong&gt;局部、通用、低级&lt;/strong&gt;的特征，例如边缘 (水平、垂直、对角线)、颜色斑块、纹理等。这些特征在不同的图像中是通用的。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;深层 (例如 VGG16 的 Conv5 或更深层)：&lt;/strong&gt; 学习到的是&lt;strong&gt;整体、抽象、高级&lt;/strong&gt;的特征，这些特征更接近于人类可以理解的语义概念。例如，它们可能检测到眼睛、鼻子、车轮、建筑物的特定部分等。这些特征通常是任务特定的，并且与最终的分类目标密切相关。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;意义：&lt;/strong&gt; 这种由浅入深的特征学习过程是卷积神经网络强大的原因之一。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;【若需查看原始图片详情，请参考原文中的“神经网络可视化示意图”】&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;2.6 应用于图像分类的卷积神经网络&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;26-应用于图像分类的卷积神经网络&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#26-%e5%ba%94%e7%94%a8%e4%ba%8e%e5%9b%be%e5%83%8f%e5%88%86%e7%b1%bb%e7%9a%84%e5%8d%b7%e7%a7%af%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;图像分类是深度学习最早取得突破性进展的应用之一，以下是一些经典的卷积神经网络模型，它们在图像分类任务上表现出色，并推动了该领域的发展：
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;AlexNet：&lt;/strong&gt; 开创性的工作，证明了深度CNN在大型数据集上的有效性。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;VGG：&lt;/strong&gt; 以其深而统一的结构 (大量堆叠 $3 \times 3$ 卷积核) 闻名，提供了深入理解网络深度的基础。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Inception 系列 (如 GoogLeNet)：&lt;/strong&gt; 引入了 Inception 模块，通过多尺度卷积核并行处理，有效地捕捉不同尺度的特征，同时控制了参数量。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;ResNet (Residual Network)：&lt;/strong&gt; 引入了残差连接 (residual connection)，解决了深度网络训练中的梯度消失和退化问题，使得网络可以训练得非常深。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;2.7 应用于目标检测的卷积神经网络&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;27-应用于目标检测的卷积神经网络&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#27-%e5%ba%94%e7%94%a8%e4%ba%8e%e7%9b%ae%e6%a0%87%e6%a3%80%e6%b5%8b%e7%9a%84%e5%8d%b7%e7%a7%af%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;目标检测是计算机视觉的核心任务之一，它不仅要识别图像中的物体类别，还要准确地标定出它们的位置。&lt;/p&gt;
&lt;h4&gt;2.7.1 任务对比 (分类 vs. 定位+分类)&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;271-任务对比-分类-vs-定位分类&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#271-%e4%bb%bb%e5%8a%a1%e5%af%b9%e6%af%94-%e5%88%86%e7%b1%bb-vs-%e5%ae%9a%e4%bd%8d%e5%88%86%e7%b1%bb&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th style=&#34;text-align: left&#34;&gt;特征&lt;/th&gt;
          &lt;th style=&#34;text-align: left&#34;&gt;图像分类 (Image Classification)&lt;/th&gt;
          &lt;th style=&#34;text-align: left&#34;&gt;目标检测 (Object Detection)&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;输入&lt;/strong&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;单一大型物体图像 (single and big object)&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;包含多个小型物体的图像 (multi and small object)&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;任务&lt;/strong&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;理解图像内容&lt;/strong&gt; (识别图像的整体类别)&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;理解图像内容&lt;/strong&gt; + &lt;strong&gt;定位图像中的物体&lt;/strong&gt; (识别物体类别并给出边界框)&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;输出&lt;/strong&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;类别标签&lt;/strong&gt; (label)&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;类别标签 &amp;amp; 边界框&lt;/strong&gt; (label &amp;amp; bounding box)&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;评价指标&lt;/strong&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;准确率&lt;/strong&gt; (precision, top-1/top-5 accuracy)&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;IoU&lt;/strong&gt; (交并比), &lt;strong&gt;mAP&lt;/strong&gt; (Mean Average Precision)&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;核心问题&lt;/strong&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;图像中有什么？&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;图像中有什么？在哪里？有多少个？&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;h4&gt;2.7.2 评测指标&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;272-评测指标&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#272-%e8%af%84%e6%b5%8b%e6%8c%87%e6%a0%87&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;h5&gt;2.7.2.1 IoU (交并比)&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;2721-iou-交并比&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#2721-iou-%e4%ba%a4%e5%b9%b6%e6%af%94&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h5&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;定义：&lt;/strong&gt; IoU (Intersection over Union) 是用于衡量&lt;strong&gt;定位准确度&lt;/strong&gt;的指标。它计算&lt;strong&gt;预测边界框 (predicted bounding box)&lt;/strong&gt; 与&lt;strong&gt;真实边界框 (ground truth bounding box)&lt;/strong&gt; 之间&lt;strong&gt;交集面积&lt;/strong&gt;与&lt;strong&gt;并集面积&lt;/strong&gt;的比值。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;计算公式：&lt;/strong&gt;
$$IoU = \frac{\text{Area of Overlap}}{\text{Area of Union}}$$&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;判断标准：&lt;/strong&gt; 通常，当 IoU &lt;strong&gt;大于或等于 0.5&lt;/strong&gt; 时，我们认为预测的边界框是&lt;strong&gt;成功的定位 (true detection)&lt;/strong&gt;。不同的数据集和任务可能会有不同的 IoU 阈值要求，例如 COCO 数据集通常使用 0.5 到 0.95 之间多个 IoU 阈值的平均值来评估。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;图示说明 (原始资料第27页图示的文字描述):&lt;/strong&gt; IoU 的计算涉及到两个矩形框，一个表示模型的预测，另一个表示真实的标注。交集是两个框重叠的部分，并集是两个框覆盖的总面积。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;【若需查看原始图片详情，请参考原文中的“IoU示意图”】&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h5&gt;2.7.2.2 mAP (Mean Average Precision 平均精度均值)&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;2722-map-mean-average-precision-平均精度均值&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#2722-map-mean-average-precision-%e5%b9%b3%e5%9d%87%e7%b2%be%e5%ba%a6%e5%9d%87%e5%80%bc&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h5&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;定义：&lt;/strong&gt; mAP 是计算机视觉领域用于&lt;strong&gt;衡量模型在测试集上检测精度优劣程度&lt;/strong&gt;的关键指标。它综合考虑了检测结果的&lt;strong&gt;召回率 (recall)&lt;/strong&gt; 和&lt;strong&gt;精度 (precision)&lt;/strong&gt;。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;特点：&lt;/strong&gt; mAP 值越高表示检测结果越好，因为它反映了模型在不同置信度阈值下的综合性能。&lt;/li&gt;
&lt;/ul&gt;
&lt;h6&gt;2.7.2.2.1 召回率与精度&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;27221-召回率与精度&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#27221-%e5%8f%ac%e5%9b%9e%e7%8e%87%e4%b8%8e%e7%b2%be%e5%ba%a6&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h6&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;召回率 / 查全率 (Recall)：&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;定义：&lt;/strong&gt; 在所有&lt;strong&gt;真实的正样本&lt;/strong&gt;中，模型成功检测到的正样本的比例。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;公式：&lt;/strong&gt; $Recall = \frac{TP}{TP + FN}$
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;TP (True Positives)：&lt;/strong&gt; 被模型正确识别为正样本的真实正样本。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;FN (False Negatives)：&lt;/strong&gt; 未被模型识别为正样本的真实正样本 (漏检)。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;直观理解：&lt;/strong&gt; 模型“找出了多少个”它应该找出来的东西。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;精度 / 查准率 (Precision)：&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;定义：&lt;/strong&gt; 在所有模型&lt;strong&gt;预测为正样本&lt;/strong&gt;的样本中，真实为正样本的比例。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;公式：&lt;/strong&gt; $Precision = \frac{TP}{TP + FP}$
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;FP (False Positives)：&lt;/strong&gt; 被模型错误识别为正样本的真实负样本 (误报)。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;直观理解：&lt;/strong&gt; 模型“找出来的东西有多少是正确的”。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;召回率与精度的权衡：&lt;/strong&gt; 召回率和精度之间通常存在此消彼长的关系。
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;提高召回率：&lt;/strong&gt; 通过降低置信度阈值，模型会检测出更多的样本，包括更多的真实正样本，从而提高召回率。但这通常也会导致更多的误报，从而降低精度。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;提高精度：&lt;/strong&gt; 通过提高置信度阈值，模型只会保留最确信的预测结果，减少误报，从而提高精度。但这可能导致漏检一些真实正样本，从而降低召回率。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h6&gt;2.7.2.2.2 mAP 计算原理与示例&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;27222-map-计算原理与示例&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#27222-map-%e8%ae%a1%e7%ae%97%e5%8e%9f%e7%90%86%e4%b8%8e%e7%a4%ba%e4%be%8b&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h6&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;计算步骤：&lt;/strong&gt;
&lt;ol&gt;
&lt;li&gt;对每个类别 (class) 分别计算&lt;strong&gt;平均精度 (Average Precision, AP)&lt;/strong&gt;。&lt;/li&gt;
&lt;li&gt;将所有类别的 AP 值&lt;strong&gt;求平均&lt;/strong&gt;，得到 mAP。&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;AP 计算原理 (VOC2012 标准，原始资料第30页和第31页图示的文字描述):&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;准备数据：&lt;/strong&gt; 假设对某个类别 A 进行检测。我们有 100 张测试图像，其中共有 25 个真实标注为 A 的物体。模型检测出了 20 个分类为 A 的候选框，每个候选框都有一个置信度分数 (confidence score) 和一个标记 (label，表示该预测是否正确，通常通过 IoU 阈值判断)。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;排序：&lt;/strong&gt; 首先，将所有预测框按照置信度分数&lt;strong&gt;从高到低排序&lt;/strong&gt;。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;计算 Precision 和 Recall 曲线：&lt;/strong&gt; 逐个遍历排序后的预测框。每增加一个预测框，就重新计算当前的 TP、FP、FN，并更新 Precision 和 Recall 值。
&lt;ul&gt;
&lt;li&gt;例如，在原始资料的示例中：
&lt;ul&gt;
&lt;li&gt;当置信度阈值设置为 0.5 时，有 4 个预测框被认为是正样本 (score &amp;gt; 0.5)。其中 3 个是 True Positive (TP)，1 个是 False Positive (FP)。那么此时 Precision = 3/4，Recall = 3/25 (总共有 25 个真实正样本)。&lt;/li&gt;
&lt;li&gt;当置信度阈值设置为 0.2 时，有 12 个预测框被认为是正样本 (score &amp;gt; 0.2)。其中 5 个是 TP。那么此时 Precision = 5/12，Recall = 5/25。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;PR 曲线：&lt;/strong&gt; 绘制 Precision-Recall 曲线，横轴为 Recall，纵轴为 Precision。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;AP 计算 (VOC2012)：&lt;/strong&gt; 对于 PR 曲线上的每个 Recall 值 (从 0 到 1)，取其对应的最大 Precision 值 (即在该 Recall 值及更高的 Recall 值中，Precision 的最大值)，然后对这些最大 Precision 值进行插值求平均。
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;更直观的看法：&lt;/strong&gt; AP 相当于 &lt;strong&gt;PR 曲线下面的面积&lt;/strong&gt;。这个面积越大，表示模型在该类别上的检测性能越好。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;code&gt;【若需查看原始图片详情，请参考原文中的“mAP计算示例表格和PR曲线图”】&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;2.7.3 基于CNN的目标检测算法分类&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;273-基于cnn的目标检测算法分类&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#273-%e5%9f%ba%e4%ba%8ecnn%e7%9a%84%e7%9b%ae%e6%a0%87%e6%a3%80%e6%b5%8b%e7%ae%97%e6%b3%95%e5%88%86%e7%b1%bb&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;目前，基于深度学习的目标检测算法大致可以分为两大类：&lt;/p&gt;
&lt;h5&gt;2.7.3.1 两阶段 (Two-stage) 算法&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;2731-两阶段-two-stage-算法&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#2731-%e4%b8%a4%e9%98%b6%e6%ae%b5-two-stage-%e7%ae%97%e6%b3%95&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h5&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;核心思想：&lt;/strong&gt; 这类算法首先&lt;strong&gt;生成候选区域 (Region Proposals)&lt;/strong&gt;，然后再对这些候选区域进行分类和边界框回归。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;过程：&lt;/strong&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;阶段一：&lt;/strong&gt; 通过一个独立的子网络 (例如 RPN, Region Proposal Network) 快速地在图像中生成一系列可能包含物体的候选框。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;阶段二：&lt;/strong&gt; 对这些候选框进行特征提取 (通常通过 RoI Pooling/Align 等操作)，然后送入分类器判断物体类别，并进一步进行边界框的精细调整 (回归)。&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;代表算法：&lt;/strong&gt; R-CNN 系列 (R-CNN, SPP-Net, Fast R-CNN, Faster R-CNN, Mask R-CNN, Cascade R-CNN 等)。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;特点：&lt;/strong&gt; 通常具有较高的检测精度，但速度相对较慢。&lt;/li&gt;
&lt;/ul&gt;
&lt;h5&gt;2.7.3.2 一阶段 (One-stage) 算法&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;2732-一阶段-one-stage-算法&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#2732-%e4%b8%80%e9%98%b6%e6%ae%b5-one-stage-%e7%ae%97%e6%b3%95&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h5&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;核心思想：&lt;/strong&gt; 这类算法直接在输入图像上&lt;strong&gt;同时预测物体的类别和边界框&lt;/strong&gt;，无需生成单独的候选区域步骤。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;过程：&lt;/strong&gt; 通过单个神经网络直接从输入图像中预测所有物体的边界框和对应的类别概率。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;代表算法：&lt;/strong&gt; YOLO (You Only Look Once) 系列 (YOLO, YOLOv2, YOLOv3, YOLOv4, YOLOv5, YOLOv6, YOLOv7, YOLOv8, YOLOX, YOLOR), SSD (Single Shot MultiBox Detector) 系列 (SSD, DSSD, DSOD, FSSD), RetinaNet, EfficientDet 等。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;特点：&lt;/strong&gt; 通常具有更快的检测速度，但精度可能略低于两阶段算法 (近年来差距正在缩小)。&lt;/li&gt;
&lt;/ul&gt;
&lt;h5&gt;2.7.3.3 发展图谱&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;2733-发展图谱&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#2733-%e5%8f%91%e5%b1%95%e5%9b%be%e8%b0%b1&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h5&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;详细分类 (原始资料第34页图示的文字描述):&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Two-stage (两阶段)：&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;R-CNN 家族：&lt;/strong&gt; R-CNN, SPP-Net, Fast R-CNN, Faster R-CNN (更好的特征网络如 HyperNet, MS-CNN, PVANet, Light-Head R-CNN; 更精确的 RPN 如 MR-CNN, FPN, CRAFT; 更完善的 ROI 分类如 R-FCN, CoupleNet, Mask R-CNN, Cascade R-CNN)。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;目标后处理：&lt;/strong&gt; OHEM (Online Hard Example Mining), Soft-NMS。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;其他：&lt;/strong&gt; A-Fast-RCNN。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;One-stage (一阶段)：&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;YOLO 家族：&lt;/strong&gt; YOLO, YOLOv2, YOLOv3, YOLOv4, YOLOv5, YOLOv6, YOLOv7, YOLOv8, YOLOX, YOLOR。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;SSD 家族：&lt;/strong&gt; SSD, R-SSD, DSSD, DSOD, FSSD。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;其他：&lt;/strong&gt; OverFeat, RetinaNet, EfficientDet。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;code&gt;【若需查看原始图片详情，请参考原文中的“目标检测算法发展图谱”】&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;2.8 应用于图像生成的卷积神经网络&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;28-应用于图像生成的卷积神经网络&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#28-%e5%ba%94%e7%94%a8%e4%ba%8e%e5%9b%be%e5%83%8f%e7%94%9f%e6%88%90%e7%9a%84%e5%8d%b7%e7%a7%af%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;h4&gt;2.8.1 生成模型与判别模型&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;281-生成模型与判别模型&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#281-%e7%94%9f%e6%88%90%e6%a8%a1%e5%9e%8b%e4%b8%8e%e5%88%a4%e5%88%ab%e6%a8%a1%e5%9e%8b&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;在深度学习中，模型可以大致分为两类：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;判别模型 (Discriminative Model)：&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;任务：&lt;/strong&gt; 主要用于&lt;strong&gt;图像识别、分类、分割&lt;/strong&gt;等任务。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;学习目标：&lt;/strong&gt; 学习输入数据 &lt;strong&gt;x&lt;/strong&gt; 到输出标签 &lt;strong&gt;y&lt;/strong&gt; 的映射关系 $P(y|x)$，即&lt;strong&gt;判别输入数据属于哪个类别&lt;/strong&gt;。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;示例：&lt;/strong&gt; 输入一张猫的图片，判别模型会输出“猫”这个标签，或者猫的概率为 0.9，狗的概率为 0.1。它只回答“是猫还是不是猫”，不关心猫本身是如何生成的。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;直观理解：&lt;/strong&gt; “Cat √ / Dog ×” (是猫，不是狗)。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;生成模型 (Generative Model)：&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;任务：&lt;/strong&gt; 主要用于&lt;strong&gt;图像生成、数据合成、数据分布学习&lt;/strong&gt;等任务。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;学习目标：&lt;/strong&gt; 学习数据本身的&lt;strong&gt;模式或特征&lt;/strong&gt;，甚至是整个&lt;strong&gt;数据分布&lt;/strong&gt; $P(x)$ 或联合分布 $P(x,y)$。一旦学习到数据的分布，就可以从这个分布中&lt;strong&gt;生成新的样本&lt;/strong&gt;。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;示例：&lt;/strong&gt; 通过学习大量猫的图片，生成模型可以生成一张全新的、逼真的猫的图片。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;直观理解：&lt;/strong&gt; 一类图像本质上是一组概率分布。生成模型的目标是找到一个模型分布 $P_\theta$ 来近似真实数据分布 $P_{data}$。一旦找到 $P_\theta$，就可以从中采样生成新数据。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;code&gt;【若需查看原始图片详情，请参考原文中的“生成模型学习数据分布示意图”】&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;2.8.2 生成对抗网络 (GAN)&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;282-生成对抗网络-gan&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#282-%e7%94%9f%e6%88%90%e5%af%b9%e6%8a%97%e7%bd%91%e7%bb%9c-gan&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;h5&gt;2.8.2.1 概述与核心思想&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;2821-概述与核心思想&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#2821-%e6%a6%82%e8%bf%b0%e4%b8%8e%e6%a0%b8%e5%bf%83%e6%80%9d%e6%83%b3&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h5&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;提出者：&lt;/strong&gt; &lt;strong&gt;Ian Goodfellow&lt;/strong&gt; 在 &lt;strong&gt;2014 年&lt;/strong&gt; 提出了生成式对抗网络 (Generative Adversarial Networks, GAN)。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;重要性：&lt;/strong&gt; 被 Yann LeCun (深度学习三巨头之一) 誉为“&lt;strong&gt;20年来机器学习领域最酷的想法&lt;/strong&gt;”。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;解决问题：&lt;/strong&gt; 从训练样本中学习出&lt;strong&gt;新的、逼真&lt;/strong&gt;的样本。为&lt;strong&gt;无监督学习和预测学习&lt;/strong&gt;提供了强大的算法框架。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;核心思想：&lt;/strong&gt; 借鉴了&lt;strong&gt;博弈论&lt;/strong&gt;的思想，通过让两个神经网络 (生成器和判别器) 相互&lt;strong&gt;对抗学习&lt;/strong&gt;，从而达到共同进步的目的。这与&lt;strong&gt;苏格拉底的反诘法&lt;/strong&gt;有异曲同工之妙。&lt;/li&gt;
&lt;/ul&gt;
&lt;h5&gt;2.8.2.2 模型组成 (生成器与判别器)&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;2822-模型组成-生成器与判别器&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#2822-%e6%a8%a1%e5%9e%8b%e7%bb%84%e6%88%90-%e7%94%9f%e6%88%90%e5%99%a8%e4%b8%8e%e5%88%a4%e5%88%ab%e5%99%a8&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h5&gt;&lt;p&gt;GAN 由两个核心部分组成，它们相互竞争：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;生成器 G (Generator) - “伪装者/造假者”：&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;输入：&lt;/strong&gt; 随机噪声 &lt;strong&gt;z&lt;/strong&gt; (通常是高斯噪声)。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;任务：&lt;/strong&gt; 找出观测数据内部的统计规律，并尝试&lt;strong&gt;生成能够以假乱真的样本 (G(z))&lt;/strong&gt;。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;目标：&lt;/strong&gt; 尽可能愚弄判别器，使判别网络输出接近 &lt;strong&gt;0.5&lt;/strong&gt; (即判别器难以区分真假样本)。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;判别器 D (Discriminator) - “警察/鉴别者”：&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;输入：&lt;/strong&gt; 真实样本 &lt;strong&gt;x&lt;/strong&gt; 或生成器生成的假样本 &lt;strong&gt;G(z)&lt;/strong&gt;。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;任务：&lt;/strong&gt; 判断输入数据是来自&lt;strong&gt;真实样本集 (real data)&lt;/strong&gt; 还是&lt;strong&gt;生成样本集 (fake data)&lt;/strong&gt;。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;目标：&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;如果输入是&lt;strong&gt;真样本 x&lt;/strong&gt;，输出接近 &lt;strong&gt;1&lt;/strong&gt; (表示“真”)。&lt;/li&gt;
&lt;li&gt;如果输入是&lt;strong&gt;生成样本 G(z)&lt;/strong&gt;，输出接近 &lt;strong&gt;0&lt;/strong&gt; (表示“假”)。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;关系：&lt;/strong&gt; 生成器和判别器构成一个&lt;strong&gt;动态的零和博弈&lt;/strong&gt;：生成器努力生成更好的假样本，判别器努力区分真假，两者在对抗中不断提升性能，直到生成器能够生成与真实数据难以区分的样本，此时判别器将无法做出准确判断（输出 0.5）。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;【若需查看原始图片详情，请参考原文中的“生成对抗网络结构示意图”】&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h5&gt;2.8.2.3 训练过程&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;2823-训练过程&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#2823-%e8%ae%ad%e7%bb%83%e8%bf%87%e7%a8%8b&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h5&gt;&lt;p&gt;GAN 的训练是一个交替迭代的优化过程：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;训练判别器 D：&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;目标：&lt;/strong&gt; 使判别器能够准确地区分真实样本和生成样本。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;输入：&lt;/strong&gt; 一批真实样本 &lt;strong&gt;x&lt;/strong&gt; (标签为 1) 和一批生成样本 &lt;strong&gt;G(z)&lt;/strong&gt; (标签为 0)。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;优化：&lt;/strong&gt; 更新判别网络的权重参数，使其在输入真实样本时输出接近 1，在输入生成样本时输出接近 0。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;训练生成器 G：&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;目标：&lt;/strong&gt; 使生成器能够生成足以欺骗判别器的假样本。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;输入：&lt;/strong&gt; 随机噪声 &lt;strong&gt;z&lt;/strong&gt;，通过生成器生成假样本 &lt;strong&gt;G(z)&lt;/strong&gt;。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;优化：&lt;/strong&gt; 更新生成网络的权重参数，使其生成的假样本 &lt;strong&gt;G(z)&lt;/strong&gt; 被判别器判断为接近 &lt;strong&gt;1&lt;/strong&gt; (即判别器认为它是真的)。这意味着生成器要最小化 $1 - D(G(z))$。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;交替迭代：&lt;/strong&gt; 这两个步骤交替进行，形成一个&lt;strong&gt;极小极大博弈 (Minimax Game)&lt;/strong&gt; 或&lt;strong&gt;零和博弈&lt;/strong&gt;。理想情况下，最终达到纳什均衡，此时生成器能生成完美的假样本，判别器对任何输入都输出 0.5。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;图示说明 (原始资料第39页图示的文字描述):&lt;/strong&gt; 展示了判别网络和生成网络在训练过程中参数更新的方向和目标。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;【若需查看原始图片详情，请参考原文中的“GAN训练过程示意图”】&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h5&gt;2.8.2.4 条件GAN (Conditional GAN, cGAN)&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;2824-条件gan-conditional-gan-cgan&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#2824-%e6%9d%a1%e4%bb%b6gan-conditional-gan-cgan&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h5&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;问题：&lt;/strong&gt; 原始 GAN 的生成器输入是纯随机噪声，这意味着它生成的样本模式是&lt;strong&gt;不可控&lt;/strong&gt;的。我们无法指定生成器生成特定类别或具有特定属性的图像。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;解决方案：&lt;/strong&gt; 条件 GAN (cGAN) 通过在生成器和判别器的输入中&lt;strong&gt;增加额外的条件信息 (category condition)&lt;/strong&gt; 来解决这个问题。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;实现：&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;生成器：&lt;/strong&gt; 输入随机噪声 &lt;strong&gt;z&lt;/strong&gt; 和条件信息 &lt;strong&gt;c&lt;/strong&gt; (例如，一个类别的 one-hot 编码向量)。生成器学习生成符合条件 &lt;strong&gt;c&lt;/strong&gt; 的样本 G(z, c)。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;判别器：&lt;/strong&gt; 输入样本 (真实样本或生成样本) 和条件信息 &lt;strong&gt;c&lt;/strong&gt;。判别器学习判断输入的样本和条件信息是否匹配 (例如，判断输入图像是否真的是一只狗，并且与条件“狗”相符)。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;效果：&lt;/strong&gt; cGAN 能够实现对生成过程的&lt;strong&gt;控制&lt;/strong&gt;，例如生成指定数字的图像、生成具有特定属性的人脸等。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;【若需查看原始图片详情，请参考原文中的“条件GAN输入输出示例图”】&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h5&gt;2.8.2.5 常见GAN结构与应用&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;2825-常见gan结构与应用&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#2825-%e5%b8%b8%e8%a7%81gan%e7%bb%93%e6%9e%84%e4%b8%8e%e5%ba%94%e7%94%a8&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h5&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;GAN 结构演变：&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;DCGAN (Deep Convolutional GAN)：&lt;/strong&gt; 将 GAN 中的全连接神经网络扩展到卷积神经网络，使得 GAN 能够处理图像数据。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;ResGAN：&lt;/strong&gt; 结合残差网络 (ResNet) 思想的 GAN，常用于图像恢复等任务。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;SRGAN：&lt;/strong&gt; 用于超分辨率重建的 GAN，同样利用了 ResNet 结构。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;CycleGAN：&lt;/strong&gt; 能够实现图像风格转换，无需成对的训练数据。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;CGAN (Conditional GAN)：&lt;/strong&gt; 如前所述，引入条件信息实现可控生成。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;InfoGAN：&lt;/strong&gt; 学习可解释的、解耦的表示，使得生成的图像在某些维度上可控。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;集成推断模型的 GAN (如 BiGAN)：&lt;/strong&gt; 尝试学习数据的逆映射，即从数据推断出潜在编码。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;对抗自编码器 (如 VAE-GAN)：&lt;/strong&gt; 结合变分自编码器 (VAE) 和 GAN 的优势。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;GAN 应用示例：&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;人脸生成：&lt;/strong&gt; 生成逼真的人脸图像，例如不存在的人脸。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;风格转换：&lt;/strong&gt; 将一张图片的风格应用到另一张图片上，或将一种图像类型转换为另一种 (如将风景照转换为梵高风格画作)。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;超分辨率 (Super Resolution)：&lt;/strong&gt; 将低分辨率图像提升到高分辨率。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;图像修复 (Inpainting)：&lt;/strong&gt; 填充图像中缺失的部分。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;数据增强：&lt;/strong&gt; 生成额外的训练数据来扩充数据集。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;文本到图像生成：&lt;/strong&gt; 根据文本描述生成图像 (例如 DALL-E 早期版本)。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;资源合集：&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;GAN zoo：https://deephunt.in/the-gan-zoo-79597dc8c347&lt;/li&gt;
&lt;li&gt;GAN 代码合集：https://github.com/zhangqianhui/AdversarialNetsPapers&lt;/li&gt;
&lt;li&gt;GAN 应用合集：https://github.com/nashory/gans-awesome-applications&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;2.8.3 扩散模型 (Diffusion Models)&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;283-扩散模型-diffusion-models&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#283-%e6%89%a9%e6%95%a3%e6%a8%a1%e5%9e%8b-diffusion-models&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;h5&gt;2.8.3.1 概述与DDPM&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;2831-概述与ddpm&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#2831-%e6%a6%82%e8%bf%b0%e4%b8%8eddpm&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h5&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;兴起：&lt;/strong&gt; 扩散模型是一种相对较新的生成模型，&lt;strong&gt;2020 年提出的 DDPM (Denoising Diffusion Probabilistic Models)&lt;/strong&gt; 在图像合成方面&lt;strong&gt;击败了 GAN&lt;/strong&gt;，显示出强大的潜力。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;核心思想：&lt;/strong&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;破坏训练数据：&lt;/strong&gt; 通过&lt;strong&gt;连续添加高斯噪声&lt;/strong&gt;来逐步破坏训练数据，使其最终变为纯噪声。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;学习恢复：&lt;/strong&gt; 模型学习&lt;strong&gt;反转噪声过程&lt;/strong&gt;，即学习如何从噪声中逐步去除噪声，从而恢复出原始数据。&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;测试时生成：&lt;/strong&gt; 在生成新数据时，首先从随机采样的&lt;strong&gt;纯高斯噪声&lt;/strong&gt;开始，然后通过模型学习到的&lt;strong&gt;去噪过程&lt;/strong&gt;逐步将其转化为清晰的数据。&lt;/li&gt;
&lt;/ul&gt;
&lt;h5&gt;2.8.3.2 标准过程 (正向扩散与逆向扩散)&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;2832-标准过程-正向扩散与逆向扩散&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#2832-%e6%a0%87%e5%87%86%e8%bf%87%e7%a8%8b-%e6%ad%a3%e5%90%91%e6%89%a9%e6%95%a3%e4%b8%8e%e9%80%86%e5%90%91%e6%89%a9%e6%95%a3&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h5&gt;&lt;p&gt;扩散模型的核心是两个相互对称的过程：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;正向扩散过程 (Forward Diffusion Process)：&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;目的：&lt;/strong&gt; 帮助神经网络训练逆向过程。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;机制：&lt;/strong&gt; 这是一个&lt;strong&gt;固定的马尔可夫链&lt;/strong&gt;过程。从原始数据 $x_0$ 开始，在每个时间步 $t$，都向当前数据 $x_{t-1}$ &lt;strong&gt;逐步添加少量高斯噪声&lt;/strong&gt;，生成 $x_t$。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;结果：&lt;/strong&gt; 经过足够多的时间步长 T 后，$x_T$ 将&lt;strong&gt;完全变成纯高斯噪声&lt;/strong&gt;，与原始数据几乎无关。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;反向逆扩散过程 (Reverse Diffusion Process)：&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;目的：&lt;/strong&gt; 这是模型需要&lt;strong&gt;学习&lt;/strong&gt;的过程，用于从噪声中生成数据。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;机制：&lt;/strong&gt; 这是一个&lt;strong&gt;可训练的马尔可夫链&lt;/strong&gt;过程。从一张纯高斯噪声图片 $x_T$ 开始，通过一个&lt;strong&gt;可训练的神经网络&lt;/strong&gt; (例如 U-net 或去噪自编码器)，在每个时间步&lt;strong&gt;预测并去除噪声&lt;/strong&gt;，逐步生成最终的清晰结果 $x_0$。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;核心：&lt;/strong&gt; 神经网络需要学习每个时间步的噪声分布，以便能够准确地“去噪”。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;code&gt;【若需查看原始图片详情，请参考原文中的“扩散模型正向/逆向过程示意图”】&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h5&gt;2.8.3.3 扩散过程&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;2833-扩散过程&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#2833-%e6%89%a9%e6%95%a3%e8%bf%87%e7%a8%8b&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h5&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;描述：&lt;/strong&gt; 正向扩散过程是将数据从清晰状态逐步过渡到完全噪声状态。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;细节：&lt;/strong&gt; 它采用一个&lt;strong&gt;固定的马尔可夫链&lt;/strong&gt;，这意味着每个时间步 $x_t$ 只依赖于前一个时间步 $x_{t-1}$。在每个时间步，都会向 $x_{t-1}$ 添加少量预定义的高斯噪声。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;最终状态：&lt;/strong&gt; 最终，当时间步 $t$ 达到 $T$ 时，$x_T$ 将成为一个&lt;strong&gt;纯粹的高斯噪声&lt;/strong&gt;，与原始图像失去了所有可识别的联系。&lt;/li&gt;
&lt;/ul&gt;
&lt;h5&gt;2.8.3.4 逆扩散过程&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;2834-逆扩散过程&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#2834-%e9%80%86%e6%89%a9%e6%95%a3%e8%bf%87%e7%a8%8b&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h5&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;描述：&lt;/strong&gt; 逆扩散过程是从纯噪声开始，逐步恢复原始数据的过程。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;细节：&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;从一张&lt;strong&gt;标准高斯噪声图片 $x_T$&lt;/strong&gt; 开始。&lt;/li&gt;
&lt;li&gt;通过一个&lt;strong&gt;可训练的网络&lt;/strong&gt; (如 U-net 或者去噪自编码器等)，该网络被训练来&lt;strong&gt;预测并减去每个时间步添加的噪声&lt;/strong&gt;。&lt;/li&gt;
&lt;li&gt;这个过程也是一个&lt;strong&gt;马尔可夫链&lt;/strong&gt;，可以被定义为 $P_\theta(x_{t-1}|x_t)$，其中 $\theta$ 是网络的参数。&lt;/li&gt;
&lt;li&gt;通过逐步迭代这个去噪过程，最终可以从 $x_T$ 生成出&lt;strong&gt;高质量的 $x_0$&lt;/strong&gt;。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h5&gt;2.8.3.5 扩散模型 vs. GANs&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;2835-扩散模型-vs-gans&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#2835-%e6%89%a9%e6%95%a3%e6%a8%a1%e5%9e%8b-vs-gans&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h5&gt;&lt;p&gt;扩散模型与生成对抗网络 (GANs) 在生成能力和训练稳定性上存在显著差异：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;生成对抗网络 (GAN) 存在的问题：&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;生成图像缺乏多样性：&lt;/strong&gt; 判别器可能会引导生成器收敛到数据分布的一个子集，导致生成样本缺乏多样性。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;模式崩溃 (Mode Collapse)：&lt;/strong&gt; 这是多样性缺乏的极端表现，生成器可能只生成少数几种模式的样本，而忽略了数据分布中的其他模式。例如，在生成人脸时，可能只生成特定表情或特定肤色的人脸。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;对抗性带来的难以训练：&lt;/strong&gt; GAN 的训练是一个不稳定的对抗过程，需要精心调整超参数，容易出现梯度消失、梯度爆炸或模式崩溃，导致训练难以收敛。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;扩散模型的优势：&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;训练更容易：&lt;/strong&gt; 扩散模型的训练过程&lt;strong&gt;没有对抗&lt;/strong&gt;，其优化目标通常更稳定，更容易收敛。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;不会受到模式崩溃的影响：&lt;/strong&gt; 由于扩散模型通过学习整个噪声分布来逐步去噪，它能够更好地覆盖整个数据分布，从而避免了模式崩溃的问题，生成样本的多样性更高。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;能够产生多样化图像：&lt;/strong&gt; 相较于 GAN，扩散模型在生成图像的多样性和质量方面往往表现更出色。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;3. 适合语音/文本处理的循环神经网络&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;3-适合语音文本处理的循环神经网络&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#3-%e9%80%82%e5%90%88%e8%af%ad%e9%9f%b3%e6%96%87%e6%9c%ac%e5%a4%84%e7%90%86%e7%9a%84%e5%be%aa%e7%8e%af%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;h3&gt;3.1 循环神经网络概述&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;31-循环神经网络概述&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#31-%e5%be%aa%e7%8e%af%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c%e6%a6%82%e8%bf%b0&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;h4&gt;3.1.1 任务特点与能力要求&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;311-任务特点与能力要求&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#311-%e4%bb%bb%e5%8a%a1%e7%89%b9%e7%82%b9%e4%b8%8e%e8%83%bd%e5%8a%9b%e8%a6%81%e6%b1%82&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;传统的神经网络（如前馈神经网络）在处理序列数据时存在局限性，因为它们假设输入是相互独立的。然而，在语音、文本等序列数据中，数据点之间往往存在&lt;strong&gt;时序上的相关性&lt;/strong&gt;，即前一个数据点会影响后一个数据点。&lt;/p&gt;
&lt;p&gt;因此，处理这类任务的网络必须具备以下能力：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;捕捉时序依赖&lt;/strong&gt;：能够理解和利用序列中不同时间步之间的关联性。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;记忆信息&lt;/strong&gt;：网络需要有“存储”信息的能力，以便在处理当前输入时能够回顾和利用之前时间步的信息。&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;3.1.2 主要应用场景&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;312-主要应用场景&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#312-%e4%b8%bb%e8%a6%81%e5%ba%94%e7%94%a8%e5%9c%ba%e6%99%af&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;循环神经网络 (RNN) 因其处理序列数据的能力，在多种领域得到了广泛应用：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;机器翻译&lt;/strong&gt;：将一种语言的句子序列翻译成另一种语言的句子序列。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;图片描述 (Image Captioning)&lt;/strong&gt;：根据输入的图片生成描述性文本序列。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;视频标注 (Video Annotation)&lt;/strong&gt;：对视频中的连续帧进行分析并生成对应的文本描述或标签序列。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;视觉问答 (Visual Question Answering)&lt;/strong&gt;：结合图像内容和自然语言问题，生成自然语言答案。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;文本生成&lt;/strong&gt;：如聊天机器人、代码生成等。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;情感分析 (Sentiment Analysis)&lt;/strong&gt;：分析文本序列（如评论）以判断其情感倾向。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;语音识别&lt;/strong&gt;：将语音信号序列转换为文本序列。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;进一步了解RNN及其应用，可参考GitHub项目：&lt;a href=&#34;https://github.com/kjw0612/awesome-rnn&#34;target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Awesome Recurrent Neural Networks&lt;/a&gt;。&lt;/p&gt;
&lt;h3&gt;3.2 循环神经网络结构&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;32-循环神经网络结构&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#32-%e5%be%aa%e7%8e%af%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c%e7%bb%93%e6%9e%84&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;h4&gt;3.2.1 核心原理&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;321-核心原理&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#321-%e6%a0%b8%e5%bf%83%e5%8e%9f%e7%90%86&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;循环神经网络通过引入&lt;strong&gt;带自反馈的神经元&lt;/strong&gt;来处理任意长度的序列数据。其核心思想是，&lt;strong&gt;隐藏状态&lt;/strong&gt; &lt;code&gt;h(t)&lt;/code&gt; 不仅与当前时刻的输入 &lt;code&gt;x(t)&lt;/code&gt; 相关，还与上一时刻的隐藏状态 &lt;code&gt;h(t-1)&lt;/code&gt; 相关。这种循环连接使得网络能够将信息从序列的前面部分传递到后面部分，从而实现对序列数据的记忆。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;时序 (Sequence)&lt;/strong&gt;：RNN能够建模序列数据，即前、后输入数据 &lt;code&gt;x(t)&lt;/code&gt; 和 &lt;code&gt;x(t+1)&lt;/code&gt; 之间不是相互独立的，而是相互影响的。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;循环 (Recurrent)&lt;/strong&gt;：对每个输入时间步执行的操作是相同的，循环往复地重复这些操作。这意味着在不同时间步，网络会&lt;strong&gt;共享&lt;/strong&gt;相同的参数 &lt;code&gt;W&lt;/code&gt; 和 &lt;code&gt;U&lt;/code&gt;。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;记忆 (Memory)&lt;/strong&gt;：隐藏层 &lt;code&gt;h(t)&lt;/code&gt; 捕捉了所有时刻 &lt;code&gt;t&lt;/code&gt; 之前的信息。理论上，&lt;code&gt;h(t)&lt;/code&gt; 记忆的内容可以无限长，但实际上受限于梯度问题，其记忆能力是有限的。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;数学表达式为：
$$h^{(t)} = f(W h^{(t-1)} + U x^{(t)} + b)$$
其中，$f$ 是非线性激活函数，常用 &lt;strong&gt;tanh&lt;/strong&gt; 或 &lt;strong&gt;ReLU&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;[原始文档中的“unfold”示意图]&lt;/code&gt;：
原始的RNN结构（左侧）在时间维度上可以展开（右侧），形成一个多层前馈网络，每一层代表序列中的一个时间步。
&lt;code&gt;【若需查看原始图片详情，请参考原文中的“循环神经网络结构”示意图】&lt;/code&gt;。&lt;/p&gt;
&lt;h4&gt;3.2.2 多种输入-输出结构&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;322-多种输入-输出结构&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#322-%e5%a4%9a%e7%a7%8d%e8%be%93%e5%85%a5-%e8%be%93%e5%87%ba%e7%bb%93%e6%9e%84&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;RNN可以根据任务需求，设计不同的输入-输出结构：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;一对多 (One-to-many)&lt;/strong&gt;：
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;输入&lt;/strong&gt;：一个单一的输入（如图片）。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;输出&lt;/strong&gt;：一个序列（如图片描述）。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;示例&lt;/strong&gt;：Image Captioning (图片描述)。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;多对一 (Many-to-one)&lt;/strong&gt;：
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;输入&lt;/strong&gt;：一个序列（如电影评论）。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;输出&lt;/strong&gt;：一个单一的输出（如情感分类）。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;示例&lt;/strong&gt;：Sentiment Analysis (情感分析)。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;多对多 (Many-to-many, 序列长度不同)&lt;/strong&gt;：
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;输入&lt;/strong&gt;：一个序列（如源语言句子）。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;输出&lt;/strong&gt;：一个不同长度的序列（如目标语言句子）。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;示例&lt;/strong&gt;：Machine Translation (机器翻译)。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;多对多 (Many-to-many, 同步)&lt;/strong&gt;：
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;输入&lt;/strong&gt;：一个序列（如视频帧）。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;输出&lt;/strong&gt;：一个相同长度的序列，每个输出对应一个输入。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;示例&lt;/strong&gt;：Video Classification (视频分类，标注每一帧)。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;多对多 (Many-to-many, 延迟)&lt;/strong&gt;：
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;输入&lt;/strong&gt;：一个序列。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;输出&lt;/strong&gt;：一个序列，但输出相对于输入有延迟（如视频标注，先看完部分视频再开始输出）。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;示例&lt;/strong&gt;：Video Caption (视频字幕)。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;值得注意的是，RNN对输入序列的长度&lt;strong&gt;没有预先定义的要求&lt;/strong&gt;，可以处理任意长度的序列。&lt;/p&gt;
&lt;h4&gt;3.2.3 正向计算过程&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;323-正向计算过程&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#323-%e6%ad%a3%e5%90%91%e8%ae%a1%e7%ae%97%e8%bf%87%e7%a8%8b&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;RNN的正向计算是从序列的第一个时间步到最后一个时间步依次进行的。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;初始时刻&lt;/strong&gt;：
&lt;ul&gt;
&lt;li&gt;通常将初始隐藏状态 $h^{(0)}$ 设为一个全零向量。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;一般时刻 $t$&lt;/strong&gt;：
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;计算隐藏状态&lt;/strong&gt;：
$$h^{(t)} = f(W h^{(t-1)} + U x^{(t)} + b)$$
其中，$f$ 是激活函数 (如 &lt;strong&gt;tanh&lt;/strong&gt; 或 &lt;strong&gt;ReLU&lt;/strong&gt;)。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;计算输出&lt;/strong&gt;：
$$o^{(t)} = V h^{(t)} + c$$&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;计算预测值&lt;/strong&gt;：
$$\hat{y}^{(t)} = \text{softmax}(o^{(t)})$$
（对于分类任务，&lt;strong&gt;softmax&lt;/strong&gt; 函数将输出归一化为概率分布）。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;在整个时间序列上，参数 $W$ (连接 $h^{(t-1)}$ 到 $h^{(t)}$)、$U$ (连接 $x^{(t)}$ 到 $h^{(t)}$)、$V$ (连接 $h^{(t)}$ 到 $o^{(t)}$) 和偏置 $b$, $c$ 都是&lt;strong&gt;共享&lt;/strong&gt;的。这意味着网络在处理序列的不同部分时使用相同的“转换规则”。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;[原始文档中的“RNN cell”示意图]&lt;/code&gt;：
该图展示了RNN在单个时间步内的计算流程。输入 $x_t$ 和前一时刻隐藏状态 $h_{t-1}$ 分别通过权重 $U$ 和 $W$ 进行线性变换，然后与偏置 $b$ 相加，经过激活函数 $f$ 得到当前时刻隐藏状态 $h_t$。$h_t$ 接着通过权重 $V$ 和偏置 $c$ 生成输出 $o_t$，最终通过 &lt;strong&gt;softmax&lt;/strong&gt; 得到预测输出 $\hat{y}_t$。
&lt;code&gt;【若需查看原始图片详情，请参考原文中的“RNN cell”示意图】&lt;/code&gt;。&lt;/p&gt;
&lt;h4&gt;3.2.4 反向传播 (BPTT - Back-Propagation Through Time)&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;324-反向传播-bptt---back-propagation-through-time&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#324-%e5%8f%8d%e5%90%91%e4%bc%a0%e6%92%ad-bptt---back-propagation-through-time&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;RNN的训练通常采用&lt;strong&gt;BPTT&lt;/strong&gt;算法，它是标准反向传播算法在时间维度上的扩展。由于RNN在时间维度上展开成一个深层网络，BPTT就是在这个展开的网络上执行反向传播。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;单个时刻 $t$ 的损失函数&lt;/strong&gt;：
$$L^{(t)} = -\sum_{j} y_j^{(t)} \ln \hat{y}_j^{(t)}$$
（这里以交叉熵损失为例，$y_j^{(t)}$ 是真实标签的one-hot编码）。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;整个序列的总损失函数&lt;/strong&gt;：
$$L = \sum_{t=1}^{\tau} L^{(t)} = -\sum_{t=1}^{\tau} \sum_{j} y_j^{(t)} \ln \hat{y}_j^{(t)}$$
其中 $\tau$ 是序列的长度。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;BPTT的关键在于计算损失函数 $L$ 对网络参数 (如 $W$, $U$, $V$) 的梯度。例如，对参数 $W$ 的偏导数需要考虑 $W$ 对所有时刻的损失 $L^{(t)}$ 的影响，并且每个时刻 $t$ 的隐藏状态 $h^{(t)}$ 都依赖于前一时刻的隐藏状态 $h^{(t-1)}$，这种依赖关系会沿着时间反向传播。&lt;/p&gt;
&lt;p&gt;计算 $W$ 的梯度时，需要将所有时间步的梯度贡献进行累加：
$$\frac{\partial L}{\partial W} = \sum_{t=1}^{\tau} \frac{\partial L^{(t)}}{\partial W}$$
其中，$\frac{\partial L^{(t)}}{\partial W}$ 需要通过链式法则沿着时间步反向展开，这导致了&lt;strong&gt;导数的递归展开&lt;/strong&gt;：
$$\frac{\partial L^{(t)}}{\partial W} = \sum_{k=1}^{t} \frac{\partial L^{(t)}}{\partial \hat{y}^{(t)}} \frac{\partial \hat{y}^{(t)}}{\partial h^{(t)}} \frac{\partial h^{(t)}}{\partial h^{(k)}} \frac{\partial h^{(k)}}{\partial W}$$
由于 $h^{(t)}$ 依赖于 $h^{(t-1)}$，这个链式法则中会包含多个 $\frac{\partial h^{(j)}}{\partial h^{(j-1)}}$ 因子。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;[原始文档中的“BPTT”示意图]&lt;/code&gt;：
此图展示了BPTT的原理。损失 $L^{(t)}$ 不仅受当前时刻的 $\hat{y}^{(t)}$ 影响，也间接地受 $h^{(t)}$ 乃至更早的 $h^{(t-1)}, h^{(t-2)}$ 的影响。因此，在计算梯度时，误差会沿着时间步反向传播，从 $L^{(\tau)}$ 传导到 $L^{(1)}$，并且每个时刻的梯度都会累加到共享参数上。
&lt;code&gt;【若需查看原始图片详情，请参考原文中的“反向传播BPTT”示意图】&lt;/code&gt;。&lt;/p&gt;
&lt;h4&gt;3.2.5 梯度消失与梯度爆炸问题&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;325-梯度消失与梯度爆炸问题&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#325-%e6%a2%af%e5%ba%a6%e6%b6%88%e5%a4%b1%e4%b8%8e%e6%a2%af%e5%ba%a6%e7%88%86%e7%82%b8%e9%97%ae%e9%a2%98&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;h5&gt;3.2.5.1 问题描述与影响&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;3251-问题描述与影响&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#3251-%e9%97%ae%e9%a2%98%e6%8f%8f%e8%bf%b0%e4%b8%8e%e5%bd%b1%e5%93%8d&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h5&gt;&lt;p&gt;由于RNN的&lt;strong&gt;递归结构&lt;/strong&gt;和BPTT算法，导致在训练过程中&lt;strong&gt;梯度消失&lt;/strong&gt;和&lt;strong&gt;梯度爆炸&lt;/strong&gt;现象比普通深度前馈网络更为明显。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;梯度消失 (Vanishing Gradient)&lt;/strong&gt;：
&lt;ul&gt;
&lt;li&gt;在BPTT计算梯度时，由于链式法则中包含多个雅可比矩阵的乘积 (如 $\frac{\partial h^{(j)}}{\partial h^{(j-1)}}$)，如果这些矩阵的范数很小（例如，激活函数（如 &lt;strong&gt;sigmoid&lt;/strong&gt; 或 &lt;strong&gt;tanh&lt;/strong&gt;）的导数在饱和区接近于零），梯度就会随着时间步的增加呈指数级衰减，导致远离输出层的早期时间步的梯度变得非常小，几乎为零。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;影响&lt;/strong&gt;：网络难以学习&lt;strong&gt;长期依赖关系&lt;/strong&gt;。例如，在一个语言模型中，预测“I grew up in Italy&amp;hellip; I speak fluent Italian.”中的“Italian.”时，需要记住很久之前的“Italy”。如果梯度消失，网络就无法将“Italy”的信息有效地传导到后面，从而无法正确预测“Italian.”。这使得RNN在处理长序列时效果不佳。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;梯度爆炸 (Exploding Gradient)&lt;/strong&gt;：
&lt;ul&gt;
&lt;li&gt;与梯度消失相反，如果雅可比矩阵的范数很大，梯度就会随着时间步的增加呈指数级增长，导致梯度变得非常大。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;影响&lt;/strong&gt;：这会导致模型参数在训练过程中更新过大，使得网络训练不稳定，甚至出现NaN值，模型无法收敛。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h5&gt;3.2.5.2 改进方法&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;3252-改进方法&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#3252-%e6%94%b9%e8%bf%9b%e6%96%b9%e6%b3%95&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h5&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;解决梯度爆炸问题&lt;/strong&gt;：
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;梯度截断 (Gradient Clipping)&lt;/strong&gt;：这是一种相对简单且有效的技术，当梯度向量的范数超过某个预设阈值时，将其按比例缩放，使其范数回到阈值以内。这可以有效防止梯度过大，稳定训练过程。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;解决梯度消失问题&lt;/strong&gt;：
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;模型结构上的改进&lt;/strong&gt;：这是更根本的解决方案，通过设计特殊的循环单元结构来缓解梯度消失问题，如&lt;strong&gt;长短期记忆网络 (LSTM)&lt;/strong&gt; 和 &lt;strong&gt;门控循环单元 (GRU)&lt;/strong&gt; 算法。这些结构通过引入“门”机制来更好地控制信息的流动，从而能够有效地学习和记忆长期依赖关系。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;3.3 长短期记忆模型 (LSTM)&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;33-长短期记忆模型-lstm&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#33-%e9%95%bf%e7%9f%ad%e6%9c%9f%e8%ae%b0%e5%bf%86%e6%a8%a1%e5%9e%8b-lstm&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;h4&gt;3.3.1 核心思想与组件&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;331-核心思想与组件&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#331-%e6%a0%b8%e5%bf%83%e6%80%9d%e6%83%b3%e4%b8%8e%e7%bb%84%e4%bb%b6&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;为了解决传统RNN的长期依赖问题（即梯度消失），&lt;strong&gt;长短期记忆网络 (Long Short-Term Memory networks, LSTM)&lt;/strong&gt; 在1997年由 Hochreiter 和 Schmidhuber 提出。&lt;/p&gt;
&lt;p&gt;LSTM的核心在于其特殊的循环单元结构，它引入了&lt;strong&gt;单元状态 (Cell State)&lt;/strong&gt; 和&lt;strong&gt;门限 (Gate)&lt;/strong&gt; 机制，使得信息可以在单元状态中长期保存，并且可以通过门来精确控制信息的流动、添加或移除。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;隐藏状态 (Hidden State)&lt;/strong&gt;：与传统RNN类似，但它更多地作为当前时间步的输出，并捕获短期信息。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;单元状态 (Cell State)&lt;/strong&gt;：这是LSTM的关键创新。它像一条&lt;strong&gt;信息传送带&lt;/strong&gt;，贯穿整个链条，能够非常容易地让信息以不变的方式向下流动。它在长时间序列中保持相关信息的能力非常强大。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;门限机制 (Gate Mechanism)&lt;/strong&gt;：LSTM通过三个主要的“门”来管理信息，每个门都是一个&lt;strong&gt;sigmoid&lt;/strong&gt;激活函数层和一个点乘操作的组合，输出一个0到1之间的数值，表示允许多少信息通过：
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;遗忘门 (Forget Gate)&lt;/strong&gt;：决定从单元状态中“遗忘”哪些信息。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;输入门 (Input Gate)&lt;/strong&gt;：决定有多少新的信息应该被添加到单元状态中。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;输出门 (Output Gate)&lt;/strong&gt;：决定当前单元状态的哪些部分应该被“输出”到隐藏状态。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;3.3.2 结构图与门限机制&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;332-结构图与门限机制&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#332-%e7%bb%93%e6%9e%84%e5%9b%be%e4%b8%8e%e9%97%a8%e9%99%90%e6%9c%ba%e5%88%b6&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;&lt;code&gt;[原始文档中的“LSTM”示意图]&lt;/code&gt;：
该图展示了一个LSTM单元在两个连续时间步 ($t$ 和 $t+1$) 的内部结构。&lt;/p&gt;
&lt;p&gt;一个LSTM单元主要由以下几个部分组成：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;遗忘门 ($f_t$)&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;接收前一时刻的隐藏状态 $h_{t-1}$ 和当前时刻的输入 $x_t$。&lt;/li&gt;
&lt;li&gt;通过一个 &lt;strong&gt;sigmoid&lt;/strong&gt; 函数输出一个介于0到1之间的向量，这个向量会与前一时刻的单元状态 $c_{t-1}$ 进行逐元素相乘。值越接近0，表示遗忘越多；值越接近1，表示保留越多。&lt;/li&gt;
&lt;li&gt;其计算公式为：$f_t = \sigma(W_f \cdot [h_{t-1}, x_t] + b_f)$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;输入门 ($i_t$) 和候选单元状态 ($\tilde{C}_t$)&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;输入门 ($i_t$)&lt;/strong&gt;：同样接收 $h_{t-1}$ 和 $x_t$，通过一个 &lt;strong&gt;sigmoid&lt;/strong&gt; 函数决定哪些新的信息需要更新到单元状态中。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;候选单元状态 ($\tilde{C}_t$)&lt;/strong&gt;：接收 $h_{t-1}$ 和 $x_t$，通过一个 &lt;strong&gt;tanh&lt;/strong&gt; 函数创建一个新的候选值向量，这些值可能会被添加到单元状态中。&lt;/li&gt;
&lt;li&gt;其计算公式为：
$i_t = \sigma(W_i \cdot [h_{t-1}, x_t] + b_i)$
$\tilde{C}&lt;em&gt;t = \text{tanh}(W_C \cdot [h&lt;/em&gt;{t-1}, x_t] + b_C)$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;更新单元状态 ($C_t$)&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;新的单元状态 $C_t$ 是由旧的单元状态 $C_{t-1}$ 经过遗忘门处理后，与输入门和候选单元状态的乘积相加而得到的。&lt;/li&gt;
&lt;li&gt;其计算公式为：$C_t = f_t * C_{t-1} + i_t * \tilde{C}_t$&lt;/li&gt;
&lt;li&gt;这个步骤是LSTM能够长期记忆信息的核心，它允许信息在时间步之间进行选择性地传递和更新。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;输出门 ($o_t$) 和隐藏状态 ($h_t$)&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;输出门 ($o_t$)&lt;/strong&gt;：接收 $h_{t-1}$ 和 $x_t$，通过一个 &lt;strong&gt;sigmoid&lt;/strong&gt; 函数决定单元状态的哪些部分将作为当前隐藏状态输出。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;隐藏状态 ($h_t$)&lt;/strong&gt;：通过将单元状态 $C_t$ 经过 &lt;strong&gt;tanh&lt;/strong&gt; 函数激活，再与输出门 $o_t$ 进行逐元素相乘而得到。&lt;/li&gt;
&lt;li&gt;其计算公式为：
$o_t = \sigma(W_o \cdot [h_{t-1}, x_t] + b_o)$
$h_t = o_t * \text{tanh}(C_t)$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;图中的 $W_f$, $W_i$, $W_o$, $W_c$ 代表了连接不同输入（$h_{t-1}$, $x_t$）到各个门的权重矩阵，它们是LSTM模型的可训练参数。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;【若需查看原始图片详情，请参考原文中的“长短期记忆模型”示意图】&lt;/code&gt;。&lt;/p&gt;
&lt;h2&gt;4. 从深度学习到大模型&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;4-从深度学习到大模型&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#4-%e4%bb%8e%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e5%88%b0%e5%a4%a7%e6%a8%a1%e5%9e%8b&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;h3&gt;4.1 注意力机制 (Attention Mechanism)&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;41-注意力机制-attention-mechanism&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#41-%e6%b3%a8%e6%84%8f%e5%8a%9b%e6%9c%ba%e5%88%b6-attention-mechanism&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;h4&gt;4.1.1 序列模型的问题与局限性&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;411-序列模型的问题与局限性&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#411-%e5%ba%8f%e5%88%97%e6%a8%a1%e5%9e%8b%e7%9a%84%e9%97%ae%e9%a2%98%e4%b8%8e%e5%b1%80%e9%99%90%e6%80%a7&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;在早期的序列模型，特别是基于循环神经网络 (RNN) 的 Seq2Seq 模型中，存在以下主要问题：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;固定长度的语义编码难以存储所有信息&lt;/strong&gt;：编码器将输入序列编码成一个固定长度的语义向量（&lt;strong&gt;上下文向量&lt;/strong&gt;）。当输入序列较长时，这个固定长度的向量很难捕获所有重要的信息，导致信息丢失，严重影响模型的性能。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;语义编码中每个元素权重相同&lt;/strong&gt;：传统的上下文向量对输入序列中的每个词或元素赋予相同的权重，模型无法区分不同部分的重要性，无法有选择性地关注与当前输出最相关的信息。&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;4.1.2 注意力机制的本质与优势&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;412-注意力机制的本质与优势&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#412-%e6%b3%a8%e6%84%8f%e5%8a%9b%e6%9c%ba%e5%88%b6%e7%9a%84%e6%9c%ac%e8%b4%a8%e4%b8%8e%e4%bc%98%e5%8a%bf&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;为了解决上述问题，注意力机制被引入。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;本质&lt;/strong&gt;：注意力机制的本质是&lt;strong&gt;分离特征的（重要性）和（内容）&lt;/strong&gt;。它允许模型在处理序列数据时，动态地为输入序列的不同部分分配不同的权重，从而有选择性地关注最相关的信息。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;工作方式&lt;/strong&gt;：与串行整合特征不同，注意力机制通过&lt;strong&gt;权重加和特征&lt;/strong&gt;来工作。它不强制将所有信息压缩到一个固定向量中，而是通过计算每个输入元素与当前任务的相关性，生成一个加权和表示。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;优势&lt;/strong&gt;：
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;抽取少量重要信息，忽略不重要信息&lt;/strong&gt;：模型能够聚焦于对当前任务有用的少量关键信息，有效地过滤掉大量不重要的或冗余的信息。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;归纳偏置&lt;/strong&gt;：注意力机制的归纳偏置在于其任务需要的特征是&lt;strong&gt;轻重有别的&lt;/strong&gt;，即并非所有输入元素都同等重要。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;提升信息处理能力&lt;/strong&gt;：它能有效提升基于 RNN（特别是 LSTM）的 Seq2Seq 模型的信息处理能力，使其能够更好地处理长序列并捕获长期依赖关系。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;类比&lt;/strong&gt;：注意力机制可以类比&lt;strong&gt;人类视觉神经系统的注意力机制&lt;/strong&gt;。当人类观察一个场景时，并不会对场景中的每个像素都进行同等程度的加工，而是会根据任务需求（例如寻找特定物体）有选择性地将注意力集中在场景中的某些特定区域，从而更高效地获取所需信息。&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;4.1.3 自注意力机制 (Self-Attention)&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;413-自注意力机制-self-attention&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#413-%e8%87%aa%e6%b3%a8%e6%84%8f%e5%8a%9b%e6%9c%ba%e5%88%b6-self-attention&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;自注意力机制是注意力机制的一种特殊形式，其中查询（Query）、键（Key）和值（Value）都来源于同一组输入。它允许模型对输入序列内部的不同位置进行加权，从而更好地捕捉序列内部的相关性。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Transformer Block&lt;/strong&gt;：自注意力机制是 &lt;strong&gt;Transformer Block&lt;/strong&gt; 的核心组成部分，而 Transformer Block 又是许多现代大型模型（如 GPT-2, GPT-3, BERT, RoBERTa, BART, T5 等）的基本构建单元。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;文本特征表示&lt;/strong&gt;：在自注意力机制中，文本特征的输入和输出仍然是&lt;strong&gt;序列长度 $\times$ 词向量维度&lt;/strong&gt;，不会发生压缩。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;QKV 概念&lt;/strong&gt;：对于输入序列中的每一个词或 token 的特征 $X$，通过线性变换生成三个部分：
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Q (Query)&lt;/strong&gt;：查询向量，表示模型正在“寻找”或“提问”的信息。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;K (Key)&lt;/strong&gt;：键向量，表示每个 token “拥有”的信息。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;V (Value)&lt;/strong&gt;：值向量，表示每个 token “提供”的信息内容。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;计算方法&lt;/strong&gt;：自注意力机制的计算过程如下：
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;计算注意力分数&lt;/strong&gt;：将查询向量 $Q$ 与所有键向量 $K$ 进行点积（内积）运算，得到一个注意力分数矩阵。这个分数衡量了查询与每个键的相似度。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;缩放&lt;/strong&gt;：为了防止点积结果过大导致 Softmax 梯度过小，将注意力分数除以 $\sqrt{d_k}$ 进行缩放，其中 $d_k$ 是键向量的维度。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;归一化&lt;/strong&gt;：对缩放后的分数应用 Softmax 函数，将其转化为概率分布，得到归一化的注意力权重。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;加权和&lt;/strong&gt;：将这些注意力权重与所有值向量 $V$ 进行加权求和，得到最终的输出。
其数学表达式为：
$$Attention(Q, K, V) = Softmax\left(\frac{QK^T}{\sqrt{d_k}}\right)V$$&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;矩阵维度&lt;/strong&gt;：
&lt;ul&gt;
&lt;li&gt;$Q$ (Query), $K$ (Key), $V$ (Value) 的维度通常为 $[L, d]$，其中 $L$ 是序列长度， $d$ 是向量维度。&lt;/li&gt;
&lt;li&gt;$QK^T$ 的维度为 $[L, d] \times [d, L] = [L, L]$。这个矩阵表示序列中每个 token 对其他所有 token 的关注度。&lt;/li&gt;
&lt;li&gt;最终输出的维度为 $[L, L] \times [L, d] = [L, d]$。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;优点与代价&lt;/strong&gt;：
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;优点&lt;/strong&gt;：自注意力机制擅长捕捉数据内部的相关性，能够有效地建模序列中任意两个位置之间的依赖关系。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;代价&lt;/strong&gt;：它会带来大量的计算和存储开销。注意力分数矩阵的维度为 $[L, L]$，这意味着计算复杂度和内存需求会随输入序列长度 $L$ 的平方增加。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;MASKED 机制&lt;/strong&gt;：在某些场景（如生成式任务，如语言建模）中，为了防止模型在预测当前 token 时“偷看”未来的 token，会使用 MASKED 机制。这意味着在计算注意力时，每个 token 的向量计算只考虑自己和之前的 tokens，不需要每个 token 的生成都重新计算所有之前的 KV 内容，通过将未来 token 的注意力分数设置为负无穷（Softmax 后变为 0）来实现。&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;4.1.4 多头注意力 (Multi-Head Attention)&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;414-多头注意力-multi-head-attention&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#414-%e5%a4%9a%e5%a4%b4%e6%b3%a8%e6%84%8f%e5%8a%9b-multi-head-attention&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;多头注意力是对自注意力机制的扩展，旨在让模型能从不同的“表征子空间”（representation sub-spaces）中学习到不同的信息。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;原理&lt;/strong&gt;：它通过并行运行多个独立的自注意力机制（称为“头”），每个头学习不同的方面或模式。最后将这些独立的注意力结果拼接起来，再经过一个线性变换，从而捕获更丰富、多样的信息。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;计算过程&lt;/strong&gt;：
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;线性变换&lt;/strong&gt;：对原始的 $Q, K, V$ 分别进行 $h$ 组不同的线性变换，生成 $h$ 组不同的 $Q_i, K_i, V_i$。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;并行计算注意力&lt;/strong&gt;：对每一组 $Q_i, K_i, V_i$ 并行计算一个自注意力结果 $head_i$。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;拼接与线性映射&lt;/strong&gt;：将 $h$ 个 $head_i$ 结果拼接 (Concatenate) 起来，然后通过一个最终的线性变换 $W^O$ 得到多头注意力的输出。
其数学表达式为：
$$MultiHead(Q, K, V) = Concat(head_1, &amp;hellip;, head_h)W^O$$
其中，$head_i = Attention(QW_i^Q, KW_i^K, VW_i^V)$。
这里，$W_i^Q, W_i^K, W_i^V$ 是第 $i$ 个头的线性变换矩阵，$W^O$ 是最终的线性变换矩阵。&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;4.2 Transformer 架构&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;42-transformer-架构&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#42-transformer-%e6%9e%b6%e6%9e%84&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;h4&gt;4.2.1 Google Transformer 结构&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;421-google-transformer-结构&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#421-google-transformer-%e7%bb%93%e6%9e%84&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;核心地位&lt;/strong&gt;：Transformer 网络是自然语言处理 (NLP) 领域中常用且影响深远的模型之一。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;与 Seq2Seq 类似&lt;/strong&gt;：其宏观结构与传统的 Seq2Seq (Encoder-Decoder) 模型类似，都包括&lt;strong&gt;编码器 (Encoder)&lt;/strong&gt; 和&lt;strong&gt;解码器 (Decoder)&lt;/strong&gt; 两个主要部分。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;组成单元&lt;/strong&gt;：然而，Transformer 完全抛弃了循环和卷积结构，转而完全基于&lt;strong&gt;注意力机制&lt;/strong&gt;（尤其是自注意力机制和多头注意力机制）来构建。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;主要应用&lt;/strong&gt;：它最初主要用于解决&lt;strong&gt;机器翻译&lt;/strong&gt;等序列到序列的任务。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;引用&lt;/strong&gt;：该模型首次由 Ashish Vaswani 等人在 2017 年的论文 &amp;ldquo;Attention is All You Need&amp;rdquo; 中提出。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;4.3 GPT 系列大型语言模型&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;43-gpt-系列大型语言模型&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#43-gpt-%e7%b3%bb%e5%88%97%e5%a4%a7%e5%9e%8b%e8%af%ad%e8%a8%80%e6%a8%a1%e5%9e%8b&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;h4&gt;4.3.1 GPT (Generative Pre-trained Transformer)&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;431-gpt-generative-pre-trained-transformer&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#431-gpt-generative-pre-trained-transformer&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;结构与潜力&lt;/strong&gt;：GPT 系列模型采用了 &lt;strong&gt;Transformer 解码器部分&lt;/strong&gt;作为其主要架构。它通过&lt;strong&gt;单向的 Block 连接&lt;/strong&gt;，使得模型在生成文本时只能依赖于其左侧（即之前）的上下文。这种设计展示了&lt;strong&gt;预训练模型在语言生成任务中的巨大潜力&lt;/strong&gt;。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;应用领域&lt;/strong&gt;：GPT 模型被广泛应用于各种文本生成任务，例如&lt;strong&gt;文本自动完成、生成对话、文章摘要&lt;/strong&gt;等。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;LLM 的自回归特性&lt;/strong&gt;：
&lt;ul&gt;
&lt;li&gt;目前的大型语言模型 (LLM) 主要以&lt;strong&gt;自回归 (autoregressive)&lt;/strong&gt; 模型为主。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;工作原理&lt;/strong&gt;：给定之前生成或输入的 token 序列 $x[1:i]$，LLM 的核心任务是&lt;strong&gt;输出下一个 token $x[i+1]$ 的概率分布&lt;/strong&gt;。通过这种方式，模型可以递归地（一个接一个地）生成整个序列，从而完成推理过程。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;计算方式&lt;/strong&gt;：通常，模型会在最后一层、对最后一个 token 的输出特征应用 Softmax 函数来计算下一个 token 的概率分布。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;GPT 的定义&lt;/strong&gt;：GPT 代表着在&lt;strong&gt;大量原始数据上得到的生成语言模型&lt;/strong&gt;。
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;架构特点&lt;/strong&gt;：原始的 GPT 架构是一个 &lt;strong&gt;12 层的 Transformer&lt;/strong&gt;。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;学习目标&lt;/strong&gt;：其主要学习目标是&lt;strong&gt;预测下一个 token&lt;/strong&gt;，即通过观察大量文本数据来学习语言的统计规律和模式。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;4.3.2 GPT 系列发展历程&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;432-gpt-系列发展历程&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#432-gpt-%e7%b3%bb%e5%88%97%e5%8f%91%e5%b1%95%e5%8e%86%e7%a8%8b&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;GPT 系列模型的发展是大型语言模型领域进步的缩影，其核心思想是利用大规模预训练和模型规模化。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;4.3.2.1 GPT-1 (2018年6月)&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;主要贡献&lt;/strong&gt;：首次提出了&lt;strong&gt;预训练 (Pre-training) 和微调 (Fine-tuning) 的统一框架&lt;/strong&gt;，奠定了后续大模型发展的基础。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;架构&lt;/strong&gt;：使用 &lt;strong&gt;Transformer 解码器结构&lt;/strong&gt;。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;数据与任务&lt;/strong&gt;：在&lt;strong&gt;大规模规范化文本语料&lt;/strong&gt;上进行预训练，之后通过微调适应&lt;strong&gt;具体的下游任务&lt;/strong&gt;（如文本分类和标注性任务）。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;规模&lt;/strong&gt;：参数量为 &lt;strong&gt;1.17 亿&lt;/strong&gt;，预训练数据量为 &lt;strong&gt;5GB&lt;/strong&gt;。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;4.3.2.2 GPT-2 (2019年2月)&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;主要贡献&lt;/strong&gt;：提出**“Language Models are Unsupervised Multitask Learners”**（语言模型是无监督的多任务学习器），强调模型通过无监督预训练就能解决各种不同的 NLP 任务，无需为每个任务单独微调。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;核心观察&lt;/strong&gt;：自然发生的任务演示，即所有 NLP 任务的示例都可以在自然文本中找到。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;数据&lt;/strong&gt;：引入了更大规模、更高质量的 &lt;strong&gt;WebText 数据集&lt;/strong&gt;（包含 800 万个网页，大小约 &lt;strong&gt;40GB&lt;/strong&gt;），极大地提升了模型的泛化能力。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;规模&lt;/strong&gt;：参数量大幅增加到 &lt;strong&gt;15 亿&lt;/strong&gt;。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;改进&lt;/strong&gt;：还增加了&lt;strong&gt;词汇表大小&lt;/strong&gt;和&lt;strong&gt;上下文窗口大小&lt;/strong&gt;。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;4.3.2.3 GPT-3 (2020年5月)&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;主要贡献&lt;/strong&gt;：提出了**“Language Models are Few-Shot Learners”&lt;strong&gt;（语言模型是少样本学习器），表明模型无需微调，仅通过与模型的文本交互（即提供少量任务相关的演示实例，称为&lt;/strong&gt;in-context learning 或 few-shot learning**）就能指定和完成任务。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;规模&lt;/strong&gt;：参数量再次爆炸性增长至 &lt;strong&gt;1750 亿&lt;/strong&gt;，是 GPT-2 的 100 多倍。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;数据&lt;/strong&gt;：预训练数据量达到惊人的 &lt;strong&gt;45TB&lt;/strong&gt;。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;In-context learning&lt;/strong&gt;：
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;与梯度下降学习的区别&lt;/strong&gt;：
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;梯度下降学习&lt;/strong&gt;：提供特征和标签作为数据，模型通过优化算法自动学习它们之间的映射关系。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;In-context learning&lt;/strong&gt;：通过在输入文本中提供示例、描述问题、提供相关信息和定义答案格式来引导模型完成任务。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;前提&lt;/strong&gt;：当模型是一个&lt;strong&gt;元学习器 (meta-learner)&lt;/strong&gt; 时，In-context learning 才成为可能。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;4.3.2.4 GPT-1/2/3 对比&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th style=&#34;text-align: left&#34;&gt;模型&lt;/th&gt;
          &lt;th style=&#34;text-align: left&#34;&gt;GPT-1&lt;/th&gt;
          &lt;th style=&#34;text-align: left&#34;&gt;GPT-2&lt;/th&gt;
          &lt;th style=&#34;text-align: left&#34;&gt;GPT-3&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;主要贡献&lt;/strong&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;提出预训练和微调的统一框架&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;用无监督预训练模型做各种不同 NLP 任务&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;无需微调，利用少量演示指定任务&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;发布时间&lt;/strong&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;2018年6月&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;2019年2月&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;2020年5月&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;参数量&lt;/strong&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;1.17 亿&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;15 亿&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;1750 亿&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;数据量&lt;/strong&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;5GB&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;40GB&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;45TB&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;4.3.2.5 CodeX (2021年8月)&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;功能&lt;/strong&gt;：一个能够&lt;strong&gt;输入自然语言，输出代码&lt;/strong&gt;的模型。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;训练方式&lt;/strong&gt;：使用 &lt;strong&gt;GitHub 上的代码数据对 GPT-3 进行微调&lt;/strong&gt;。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;数据集&lt;/strong&gt;：训练数据集为 &lt;strong&gt;159GB 从 GitHub 上筛选的代码数据&lt;/strong&gt;。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;能力&lt;/strong&gt;：它“精通”Python、JavaScript、PHP、Swift、Shell 等多种编程语言。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;4.3.2.6 InstructGPT (2022年3月)&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;主要贡献&lt;/strong&gt;：InstructGPT 提出并使用了&lt;strong&gt;来自人类反馈的强化学习 (Reinforcement Learning from Human Feedback, RLHF)&lt;/strong&gt; 对预训练大模型进行微调，显著提升了模型遵循指令的能力和安全性。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;三步微调流程&lt;/strong&gt;：
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;监督方式微调 (Supervised Fine-Tuning, SFT)&lt;/strong&gt;：收集人类标注的指令数据，以监督学习的方式微调 GPT-3 模型。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;训练奖励模型 (Reward Model, RM)&lt;/strong&gt;：利用人类对模型生成结果的偏好排序数据，训练一个奖励模型。这个奖励模型能够评估模型输出的质量。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;强化学习微调 (Reinforcement Learning)&lt;/strong&gt;：利用训练好的奖励模型提供 reward 信号，使用近端策略优化 (Proximal Policy Optimization, PPO) 算法对第一步微调的 GPT-3 模型进行强化学习微调。&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;4.3.2.7 ChatGPT (2022年11月)&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;训练语料&lt;/strong&gt;：其训练语料包含了多种主题的数据，使其能够处理各种不同任务，包括&lt;strong&gt;回答问题、撰写文章、多语种翻译、生成代码&lt;/strong&gt;等。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;连续对话&lt;/strong&gt;：ChatGPT 能够&lt;strong&gt;主动记忆之前的对话内容&lt;/strong&gt;，实现流畅的&lt;strong&gt;连续对话&lt;/strong&gt;。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;训练方法&lt;/strong&gt;：与 InstructGPT 类似，ChatGPT 也使用了&lt;strong&gt;来自人类反馈的强化学习 (RLHF)&lt;/strong&gt; 对预训练大模型进行微调。这提升了模型理解人类思维的准确性，并使其功能优于纯粹的 GPT-3。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;与 InstructGPT 的关系&lt;/strong&gt;：ChatGPT 和 InstructGPT 在训练流程上是相同的。主要区别在于 ChatGPT 在标注数据环节新增了大量的&lt;strong&gt;对话形式数据&lt;/strong&gt;，并将原来的数据也全部改为了对话形式，使其更适合对话交互。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;4.3.2.8 GPT-4 (2023年3月)&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;多模态能力&lt;/strong&gt;：GPT-4 是一款&lt;strong&gt;多模态模型&lt;/strong&gt;，可以&lt;strong&gt;同时接收图像输入和文本输入&lt;/strong&gt;。它在多种多模态任务上的 zero-shot (零样本) 效果达到了甚至超过了针对性训练的模型。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;上下文长度&lt;/strong&gt;：上下文长度大幅提升，可达 &lt;strong&gt;32768 tokens&lt;/strong&gt;，相当于一次可以处理超过 50 页的内容，极大地扩展了模型的理解和生成能力。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;训练与性能&lt;/strong&gt;：GPT-4 沿用了与 ChatGPT 一致的训练方法 (RLHF)。但在&lt;strong&gt;文本总结和加工能力&lt;/strong&gt;上有了明显的提升，能够根据指令给出更优质、更准确的答案。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;4.4 大语言模型 (LLM) 部署过程&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;44-大语言模型-llm-部署过程&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#44-%e5%a4%a7%e8%af%ad%e8%a8%80%e6%a8%a1%e5%9e%8b-llm-%e9%83%a8%e7%bd%b2%e8%bf%87%e7%a8%8b&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;大型语言模型从一个未经训练的神经网络到能够智能响应用户指令的完整系统，通常需要经过以下三个阶段：&lt;/p&gt;
&lt;h4&gt;4.4.1 预训练 (Pre-Training)&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;441-预训练-pre-training&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#441-%e9%a2%84%e8%ae%ad%e7%bb%83-pre-training&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;目标&lt;/strong&gt;：通过&lt;strong&gt;大规模无监督学习&lt;/strong&gt;，让模型在海量文本数据上学习语言的&lt;strong&gt;结构、知识和模式&lt;/strong&gt;。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;方法&lt;/strong&gt;：通常通过&lt;strong&gt;语言建模任务&lt;/strong&gt;（如预测下一个词）来训练模型，使其掌握通用的语言理解能力。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;结果&lt;/strong&gt;：预训练后的模型能够&lt;strong&gt;生成合理的文本段落&lt;/strong&gt;，但对特定任务或指令的理解和执行能力仍然有限，可能无法直接用于与用户交互。&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;4.4.2 监督微调 (Supervised Fine-Tuning, SFT)&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;442-监督微调-supervised-fine-tuning-sft&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#442-%e7%9b%91%e7%9d%a3%e5%be%ae%e8%b0%83-supervised-fine-tuning-sft&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;目标&lt;/strong&gt;：通过&lt;strong&gt;监督学习&lt;/strong&gt;，引导模型执行更为具体的任务，让其能够更好地理解和响应用户指令。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;方法&lt;/strong&gt;：使用高质量、标注好的&lt;strong&gt;指令-响应对数据&lt;/strong&gt;来调整模型参数。这些数据可以是问答、代码生成、文本摘要等各种任务的示例。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;作用&lt;/strong&gt;：使模型能够更准确、更相关地执行特定任务。Instruct Tuning 和 InstructGPT 就是这种方法的典型代表。&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;4.4.3 偏好优化 (Preference Optimization)&lt;span class=&#34;hx:absolute hx:-mt-20&#34; id=&#34;443-偏好优化-preference-optimization&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#443-%e5%81%8f%e5%a5%bd%e4%bc%98%e5%8c%96-preference-optimization&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;目标&lt;/strong&gt;：通过&lt;strong&gt;强化学习&lt;/strong&gt;，进一步调整模型的生成行为，使其输出&lt;strong&gt;更符合人类的期望、偏好和价值观&lt;/strong&gt;。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;方法&lt;/strong&gt;：这个阶段通常涉及&lt;strong&gt;人类反馈强化学习 (RLHF)&lt;/strong&gt;。模型会生成多个响应，由人类标注者进行评估和排序，然后训练一个奖励模型来学习人类偏好。最后，利用奖励模型提供的 reward 信号，通过强化学习算法（如 PPO）来优化模型的策略，使其生成更受人类偏好的输出。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;作用&lt;/strong&gt;：经过偏好优化后的模型在对话、回答问题、生成内容时，其输出会&lt;strong&gt;更符合人类期望，减少不准确、有偏见或不合适的输出&lt;/strong&gt;。ChatGPT 和 Claude 等模型都广泛采用了这一技术。&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
  </channel>
</rss>
